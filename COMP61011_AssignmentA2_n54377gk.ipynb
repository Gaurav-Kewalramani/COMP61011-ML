{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdjMX2t_0zCR"
      },
      "source": [
        "# Assignment Brief: Neural Networks in PyTorch for Machine Learning  \n",
        "\n",
        "## Deadline: 01 November 2024, 14:00 GMT\n",
        "\n",
        "## Number of marks available: 10\n",
        "\n",
        "In this practical, we will practice using PyTorch to build a neural network to carry out the MNIST classification tasks, specifically writing the optimiser classes needed to train the network.\n",
        "\n",
        "### Please READ the whole assignment first, before starting to work on it.\n",
        "\n",
        "### How and what to submit\n",
        "\n",
        "A. A **Jupyter Notebook** with the code in all the cells executed and outputs displayed, together with your saved models and training logs (the text output from the `train()` function). The training logs can be included in the final, executed notebook, or saved separately as a `.txt` file.\n",
        "\n",
        "B. Name your Notebook **COM61011_AssignmentA2_XXXXXX.ipynb** where XXXXXX is your username such as such as abc18de. Example: `COM61011_AssignmentA2_abc18de.ipynb`\n",
        "\n",
        "C. Upload the Jupyter Notebook in B to Blackboard under the **Group A: Computing Assignment 2** submission area before the deadline. **There are two submissions: please pay close attention to submit to the right place!**\n",
        "\n",
        "D. **NO DATA UPLOAD**: Please do not upload the data files used in this Notebook. We have a copy already.\n",
        "\n",
        "\n",
        "### Running this code\n",
        "\n",
        "By default, this code requires a GPU with software called \"CUDA\" to run. If you do not have a CUDA-enabled GPU, you can run this code on Google Colab. You can also set the `device` variable to `cpu` to run this code on your CPU, but it may be very slow.\n",
        "\n",
        "\n",
        "### Assessment Criteria\n",
        "\n",
        "* Successful implementation, from scratch, of various optimisers and weight update equations.\n",
        "\n",
        "* Being able to write an optimiser class that functions within a PyTorch training loop.\n",
        "\n",
        "* Being able to successfully train a neural network using PyTorch and save the resulting model.\n",
        "\n",
        "\n",
        "### Code quality and use of Python libraries\n",
        "When writing your code, you will find out that there are operations that are repeated at least twice. If your code is unreadable, we may not award marks for that section. Make sure to check the following:\n",
        "\n",
        "* Did you include Python functions to solve the question and avoid repeating code?\n",
        "* Did you comment your code to make it readable to others?\n",
        "\n",
        "**DO NOT USE the import `torch.optim` for the questions on this assignment. You are meant to write Python code from scratch. Using pre-built optimisers for the questions on this assignment will give ZERO marks. No excuse will be accepted.**\n",
        "\n",
        "Furthermore, please try to avoid using any imports apart from the ones already provided in the Notebook. You can easily install all recommended modules for this assignment by running the following command in your terminal: `python -m pip install -r requirements.txt`\n",
        "\n",
        "\n",
        "### Late submissions\n",
        "\n",
        "We follow Department's guidelines about late submissions, i.e., a deduction of 10% of the mark each 24 hours the work is late after the deadline. NO late submission will be marked one week after the deadline. Please read [this link](https://wiki.cs.manchester.ac.uk/index.php/UGHandbook23:Main#Late_Submission_of_Coursework_Penalty).\n",
        "\n",
        "### Use of unfair means\n",
        "\n",
        "**Any form of unfair means is treated as a serious academic offence and action may be taken under the Discipline Regulations.** Please carefully read [what constitutes Unfair Means](https://documents.manchester.ac.uk/display.aspx?DocID=2870) if not sure. If you still have questions, please ask your Personal tutor or the Lecturers.\n",
        "\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asDZDvGrhMyk"
      },
      "source": [
        "# Background: Optimisers in Deep Learning\n",
        "A key component of deep learning is the iterative weight update equation, often called an optimiser. The optimiser is responsible for updating the weights of the network in order to minimise the loss function, and the choice of optimiser can have a significant impact on the performance of the model. In this coursework, you will be implement from scratch some popular optimisers and compare their performance in a simple neural network classifying the MNIST dataset.\n",
        "\n",
        "**Task**: to implement the following optimisers for MNIST classification tasks: **AdaGrad** (Adaptive gradient), **RMSProp** (Root mean square propagation), and **Adam** (Adaptive moments).\n",
        "\n",
        "**Data**: MNIST dataset, which contains 60,000 training images and 10,000 testing images of handwritten digits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MueN_DBim2cI"
      },
      "source": [
        "### Optimiser recap\n",
        "We first have a quick recap of the basic ideas in each of the optimisers.  \n",
        "\n",
        "* Stochastic gradient descent (one data point at a time):\n",
        "    $$ \\begin{align*}\n",
        "        w^{(\\tau)} &= w^{(\\tau-1)} + \\Delta w^{(\\tau-1)}, \\\\\n",
        "        \\Delta w^{(\\tau-1)} &= - \\eta \\nabla E_{n}(w^{\\tau-1})\n",
        "    \\end{align*} $$\n",
        "\n",
        "* SGD with momentum:\n",
        "    $$\\begin{align*}\n",
        "        w^{(\\tau)} &= w^{(\\tau-1)} + \\Delta w^{(\\tau-1)}, \\\\\n",
        "        \\Delta w^{(\\tau-1)} &= - \\eta \\left[\\nabla E_{n}(w^{\\tau-1}) + \\mu \\Delta w^{(\\tau-2)}\\right]\n",
        "    \\end{align*} $$\n",
        "    where $\\mu$ ('mu') is called the *momentum parameter*.\n",
        "\n",
        "* AdaGrad (Adaptive gradient): to reduce each learning rate by using the accumulated sum of squared gradients\n",
        "$$    \\begin{align*}\n",
        "        r_i^{(\\tau)} &= r_i^{(\\tau-1)} + \\left(\\frac{\\partial E(w)}{\\partial w_i}\\right)^2, \\\\\n",
        "        w_i^{(\\tau)} &= w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{r_i^{(\\tau)}} + \\epsilon}\\frac{\\partial E(w)}{\\partial w_i}\n",
        "    \\end{align*} $$\n",
        "\n",
        "* RMSProp (Root mean square propagation): moving average of the squared gradient\n",
        "    $$ \\begin{align*}\n",
        "        r_i^{(\\tau)} &= \\beta r_i^{(\\tau-1)} + (1-\\beta) \\left(\\frac{\\partial E(w)}{\\partial w_i}\\right)^2, \\\\\n",
        "        w_i^{(\\tau)} &= w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{r_i^{(\\tau)}} + \\epsilon}\\frac{\\partial E(w)}{\\partial w_i}\n",
        "    \\end{align*}$$\n",
        "    where $\\beta$ ('beta') is called the *decay rate* of the moving average.\n",
        "\n",
        "* Adam (Adaptive moments): moving average for both the gradient and the squared gradient\n",
        "    $$ \\begin{align*}\n",
        "        s_i^{(\\tau)} &= \\beta_1 s_i^{(\\tau-1)} + (1-\\beta_1) \\frac{\\partial E(w)}{\\partial w_i}, \\\\\n",
        "        r_i^{(\\tau)} &= \\beta_2 r_i^{(\\tau-1)} + (1-\\beta_2) \\left(\\frac{\\partial E(w)}{\\partial w_i}\\right)^2 \\\\\n",
        "        \\hat{s_i}^{(\\tau)} &= \\frac{s_i^{(\\tau)}}{1-\\beta_1^{\\tau}}, \\\\\n",
        "        \\hat{r_i}^{(\\tau)} &= \\frac{r_i^{(\\tau)}}{1-\\beta_2^{\\tau}}, \\\\\n",
        "        w_i^{(\\tau)} &= w_i^{(\\tau-1)} - \\frac{\\eta}{\\sqrt{\\hat{r_i}^{(\\tau)}} + \\epsilon}\\hat{s_i}^{(\\tau)}\n",
        "    \\end{align*}$$\n",
        "    where $\\beta_1, \\beta_2$ are the *decay rates* of the moving averages, and with $\\beta_1^{\\tau}, \\beta_2^{\\tau}$ we denote $\\beta_1, \\beta_2$ to the power $\\tau$.\n",
        "\n",
        "In all cases, $\\eta$ is the learning rate, and $\\epsilon$ is a small number to avoid division by zero improve numerical stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exlrRQZIk_F5"
      },
      "source": [
        "### Hyperparameter settings\n",
        "\n",
        "In your implementations, please use the following hyperparameter setups: $\\epsilon=1e-6$, $\\mu=0.9$, $\\eta=0.01$, $\\beta=0.99$, $\\beta_1=0.9$, $\\beta_2=0.99$. (DO NOT CHANGE ANY OF THEM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-vMdsMDUnEct"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# import torch.optim as optim (this is NOT allowed; you can't use torch optimizers)\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "batch_size = 64\n",
        "test_batch_size = 1000\n",
        "lr = 0.001\n",
        "seed = 10\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_kwargs = {'batch_size': batch_size}\n",
        "test_kwargs = {'batch_size': test_batch_size}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLBmCQkEmAGT"
      },
      "source": [
        "and the following convnet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mfid75ZjhJhl"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqJ3hfdvnmSH"
      },
      "source": [
        "We also provide the test function to report the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sHSmarGMml99"
      },
      "outputs": [],
      "source": [
        "# for test accuracy report\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8xdWWRe0zCW"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "MNIST is a well-known dataset of images of handwritten digits. A copy of the MNIST dataset is included in PyTorch. We will download it, then load it into memory using the `torchvision` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HF6TdEeF0zCW"
      },
      "outputs": [],
      "source": [
        "# The values 0.1307 and 0.3081 used for the Normalize() transformation below are the global mean and standard\n",
        "# deviation of the MNIST dataset. This is equivalent to scaling all pixel values between [0, 1].\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "dataset2 = datasets.MNIST('./data', train=False, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "2SF3vkev0zCX",
        "outputId": "38215fdd-45bd-40cc-860d-b6e1836e9b89"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 16 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxkAAAMWCAYAAACdtUsqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAJElEQVR4nO3ceZiWddnw8XtGQFEHcHsMVNxxI0XRXDI1wyVTXMqFXNJKzZ7UTJQWM0vNJTUVMreiXNIsFyw1Iddcc8kKEUVKFkdzBcYFUOd+/3iO9+jtec+LmWs8Z+57Zj6fP7/HfVz3T70uhpNLzoZqtVqtAAAAJGms9QEAAICexZABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQKo+7flQa2trpbm5udLU1FRpaGjo7DNBu1Wr1UpLS0tlyJAhlcbG2s7MnhPqWb08K54T6lm9PCeVimeF+tXe56RdQ0Zzc3NljTXWSDscZJszZ05l9dVXr+kZPCd0B7V+VjwndAe1fk4qFc8K9a+t56RdY3pTU1PagaAz1MM9Wg9ngLbU+j6t9fdDe9TDfVoPZ4AlaesebdeQ4TUd9a4e7tF6OAO0pdb3aa2/H9qjHu7TejgDLElb96i/+A0AAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqk+tDwBQ1siRI8P+ta99LeyHHXZY2K+66qqwjx8/PuxPPvlkO04HAHiTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACksl0qyVJLLRX2gQMHply/aGvOsssuG/YNNtgg7P/93/8d9vPOOy/sY8aMCfvChQvDfvbZZ4f9+9//fthhSUaMGBH2KVOmhH3AgAFhr1arYT/00EPDPnr06LCvtNJKYQf+7VOf+lTYr7322rDvuOOOYX/22WfTzgRd4ZRTTgl70e+BGhvjP+vfaaedwn7fffd16Fy14k0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJCq12yXGjp0aNj79esX9u222y7s22+/fdgHDRoU9s9+9rNtH64TzJ07N+wXX3xx2Pfdd9+wt7S0hP2vf/1r2Lvb5gPqw8c+9rGw33jjjWEv2tpWtEWq6D5evHhx2Iu2SG2zzTZhf/LJJ0tdn861ww47hL3ov+vNN9/cmcfpdbbaaquwP/bYY118Eugchx9+eNjHjRsX9tbW1lLXL/pZ1t14kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApOpx26VGjBgR9rvvvjvsRVtquouijQWnnHJK2N96662wX3vttWF/6aWXwv7mm2+G/dlnnw07vcuyyy4b9i222CLs11xzTdgHDx6ccp4ZM2aE/dxzzw379ddfH/YHH3ww7EXP21lnndWO05Ftp512Cvv6668fdtulOqaxMf5zyrXXXjvsa665ZtgbGhrSzgRdoeheXmaZZbr4JPXNmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWP2y41e/bssL/++uthr9V2qUcffTTs8+bNC/snP/nJsC9evDjsV199dYfOBRkuu+yysI8ZM6aLT/I/irZaLb/88mG/7777wl60tWjTTTft0LnoHIcddljYH3744S4+Sc9WtP3tyCOPDHvRFrnp06ennQkyjRo1KuzHHntsqesU3eN77rln2P/1r3+Vun698iYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjV47ZLvfHGG2E/6aSTwl70N/v/8pe/hP3iiy8udZ6nnnoq7LvsskvY33777bBvsskmYT/++ONLnQcyjRw5Muyf+cxnwt7Q0FDq+kVbnn73u9+F/bzzzgt7c3Nz2Iue8zfffDPsO++8c9jL/nPRuRob/flZV7jyyitLfX7GjBmddBL4cLbffvuwT5w4MexlN5P+6Ec/CvusWbNKXae78SsxAACQypABAACkMmQAAACpDBkAAEAqQwYAAJCqx22XKnLLLbeE/e677w57S0tL2DfbbLOwf+lLXwp70baboi1SRZ5++umwH3XUUaWuAx0xYsSIsE+ZMiXsAwYMCHu1Wg37HXfcEfYxY8aEfccddwz7KaecEvaiLTivvvpq2P/617+GvbW1NexF27S22GKLsD/55JNhp5xNN9007KuuumoXn6R3Krthp+jXC6i1L3zhC2EfMmRIqevce++9Yb/qqqvKHqlH8CYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjVa7ZLFVmwYEGpz8+fP7/U54888siw//rXvw570fYa6ArDhg0L+0knnRT2ou0yr732WthfeumlsP/yl78M+1tvvRX22267rVTvbP379w/7iSeeGPaDDz64M4/Ta+yxxx5hL/rvQccUbetae+21S13nxRdfzDgOdNjKK68c9i9+8YthL/o92bx588J+xhlndOhcPZU3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAql6/Xaqs0047LewjR44M+4477hj2UaNGhX3y5MkdOheUsfTSS4f9vPPOC3vRFp+WlpawH3bYYWF//PHHw95TtwENHTq01kfo0TbYYINSn3/66ac76SQ9W9GvC0Vbp5577rmwF/16AdnWWmutsN94440p1x8/fnzY77nnnpTr9xTeZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqWyXKuntt98O+5FHHhn2J598MuxXXHFF2Is2ExRt5fnJT34S9mq1GnaoVCqVzTffPOxFW6SK7L333mG/7777Sp8JOttjjz1W6yN0qQEDBoR99913D/shhxwS9l133bXU955++ulhnzdvXqnrQEcV3eObbrppqevcddddYb/oootKn6k38iYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1SSmTNnhv3www8P+8SJE8N+6KGHlurLLbdc2K+66qqwv/TSS2Gnd7ngggvC3tDQEPaibVG9bYtUY2P85zKtra1dfBI6YsUVV+zU62+22WZhL3quRo0aFfbVV1897P369Qv7wQcfHPai+/Xdd98N+6OPPhr2RYsWhb1Pn/i3EE888UTYIds+++wT9rPPPrvUdR544IGwf+ELXwj7/PnzS12/t/ImAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUJ7v55pvDPmPGjLAXbf351Kc+FfYf/vCHYV9zzTXDfuaZZ4b9xRdfDDvd25577hn2ESNGhL1arYb91ltvzTpSt1a0Raro39tTTz3ViaehaEtS0X+PSy+9NOzf/va3U86z6aabhr1ou9T7778f9nfeeSfs06ZNC/vPf/7zsD/++ONhL9oK969//Svsc+fODXv//v3DPn369LBDR6211lphv/HGG1Ou/49//CPsRc8E7eNNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQynapGpk6dWrYDzjggLDvtddeYZ84cWLYjz766LCvv/76Yd9ll13CTvdWtP2lX79+YX/llVfC/utf/zrtTPVk6aWXDvtpp51W6jp333132L/1rW+VPRIlfPWrXw37rFmzwr7ddtt15nEqs2fPDvstt9wS9meeeSbsjzzySNaRSjnqqKPCvsoqq4S9aCMPZBs3blzYizb+lXX22WenXIf/5E0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqk6M2/evLBfffXVYb/yyivD3qdP/J92hx12CPtOO+0U9nvvvTfs9EyLFi0K+0svvdTFJ8lVtEXqlFNOCftJJ50U9rlz54b9/PPPD/tbb73VjtOR7Zxzzqn1EbqlT33qU6U+f+ONN3bSSeitRowYEfZdd9015fqTJk0K+7PPPptyff6TNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpWpk0003DfvnPve5sG+11VZhL9oiVWTatGlhv//++0tdh57p1ltvrfURPpSizSRF26IOPPDAsBdtIPnsZz/boXNBT3TzzTfX+gj0MJMnTw77CiusUOo6jzzySNgPP/zwskfiQ/AmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUkg022CDsX/va18K+3377hf0jH/lIynk++OCDsL/00kthb21tTfle6ktDQ0Opvs8++4T9+OOPzzpSihNOOCHs3/3ud8M+cODAsF977bVhP+ywwzp2MAA6bKWVVgp72d+jXHLJJWF/6623Sp+JjvMmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUgaItT2PGjAl70RaptdZaK+tIoccffzzsZ555ZthvvfXWzjwOdaZarZbqRff9xRdfHPaf//znYX/99dfDvs0224T90EMPDftmm20W9tVXXz3ss2fPDvudd94Z9qINJMC/FW2jGzZsWNgfeeSRzjwOPcDEiRPD3tiY82ffDz30UMp1+HC8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUvWa7VKrrrpq2DfeeOOwT5gwIewbbrhh2pkijz76aNh/9KMfhX3SpElhb21tTTsTvcdSSy0V9q9+9ath/+xnPxv2BQsWhH399dfv2MH+l6LNIffcc0/YTz311JTvhd6oaBtd1iYgeq4RI0aEfdSoUWEv+r3L4sWLw/6Tn/wk7P/617/aPhydzq8QAABAKkMGAACQypABAACkMmQAAACpDBkAAECqbrtdasUVVwz7ZZddFvaiDQfrrLNO1pFCRVtwzj///LDfeeedYX/33XfTzkTv8fDDD4f9scceC/tWW21V6vof+chHwl60za3I66+/Hvbrr78+7Mcff3yp6wP5tt1227D/4he/6NqDULcGDRoU9qKfHUVefPHFsI8dO7bskehC3mQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKnqZrvU1ltvHfaTTjop7B/72MfCvtpqq6WdKfLOO++E/eKLLw77D3/4w7C//fbbaWeCInPnzg37fvvtF/ajjz467KecckrKeS666KKw//SnPw37888/n/K9QMc1NDTU+ghAN+RNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqm62S+27776lelnTpk0L++9///uwv//++2E///zzwz5v3rwOnQtq4aWXXgr7aaedVqoDPccdd9wR9v3337+LT0JPMX369LA/9NBDYd9+++078zh0MW8yAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUDdVqtdrWhxYsWFAZOHBgV5wHOmT+/PmVAQMG1PQMnhO6g1o/K54TuoNaPyeVimeF+tfWc+JNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqnYNGdVqtbPPAR9KPdyj9XAGaEut79Nafz+0Rz3cp/VwBliStu7Rdg0ZLS0tKYeBzlIP92g9nAHaUuv7tNbfD+1RD/dpPZwBlqSte7Sh2o5RubW1tdLc3FxpamqqNDQ0pB0OPqxqtVppaWmpDBkypNLYWNv/+89zQj2rl2fFc0I9q5fnpFLxrFC/2vuctGvIAAAAaC9/8RsAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUvVpz4daW1srzc3NlaampkpDQ0NnnwnarVqtVlpaWipDhgypNDbWdmb2nFDP6uVZ8ZxQz+rlOalUPCvUr/Y+J+0aMpqbmytrrLFG2uEg25w5cyqrr756Tc/gOaE7qPWz4jmhO6j1c1KpeFaof209J+0a05uamtIOBJ2hHu7RejgDtKXW92mtvx/aox7u03o4AyxJW/dou4YMr+mod/Vwj9bDGaAttb5Pa/390B71cJ/WwxlgSdq6R/3FbwAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUfWp9AADg3y666KKwH3fccWGfOnVq2Pfcc8+wz5o1q2MHAyjBmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJXtUkC309TUFPbll18+7J/5zGfCvsoqq4T9ggsuCPuiRYvacTpon7XWWivshxxySNhbW1vDvtFGG4V9ww03DLvtUnQ3w4YNC3vfvn3DvsMOO4T9kksuCXvRs9XZJk2aFPaDDjoo7IsXL+7M46TzJgMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGW7FFBzRVt2xo0bF/Ztt9027MOHD085z+DBg8N+3HHHpVwfKpVK5dVXXw37/fffH/bRo0d35nGgy2yyySZhP/zww8O+//77h72xMf6z8iFDhoS9aItUtVoNe2creqYvvfTSsH/9618P+4IFC7KOlMqbDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAgle1SSbbeeuuwH3LIIWHfcccdw160caHI2LFjw97c3Bz27bffPuzXXHNN2B999NFS54FKpVLZcMMNw160GePggw8Oe//+/cPe0NAQ9jlz5oS9paUl7BtttFHYDzjggLBfcsklYZ8+fXrYYUnefvvtsM+aNauLTwJd66yzzgr7Hnvs0cUnqU+HHXZY2H/2s5+F/cEHH+zM43SYNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpUo68MADw37RRReFfeWVVw570Xace++9N+yrrLJK2H/0ox+FvUjR9xZd/6CDDip1fXqmgQMHhv2cc84Je9Fz0tTUlHKeGTNmhH233XYLe9++fcNetBWq6Lkt6tARgwYNCvtmm23WtQeBLjZlypSwl90u9corr4S9aAtTY2P8Z+utra2lvne77bYLe9Hm0N7KmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWv3y7Vp0/8r2DLLbcM+xVXXBH2ZZddNuz3339/2E8//fSwP/DAA2Ffeumlw37DDTeEfddddw17kccff7zU5+ld9t1337B/+ctf7tTvnTlzZth32WWXsM+ZMyfs6623XtqZIEvRz42hQ4emXH+rrbYKe9FWtVmzZqV8L7Tlpz/9adhvueWWUtd57733wv7yyy+XPVIpAwYMCPvUqVPDPmTIkFLXL/r30N1+r+ZNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqtdvlzrkkEPCfuWVV5a6zpQpU8J+4IEHhn3BggWlrl90nbJbpObOnRv2X/7yl6WuQ++y//77p1znhRdeCPtjjz0W9nHjxoW9aItUkY022qjU56ErNDc3h/0Xv/hF2E877bRS1y/6/Lx588I+YcKEUteHjnr//ffDXvbX9lrZbbfdwr7CCiukXL/o92qLFi1KuX5X8SYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjVa7ZLnX766WH/9re/HfZqtRr2Sy65JOynnHJK2MtukSryne98J+U6xx13XNhfffXVlOvTMx155JFhP+qoo8I+efLksD///PNhf+WVVzp2sHZaddVVO/X6kKno51XZ7VLAh3PQQQeFvehnYv/+/VO+99RTT025Tq15kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApOpx26WK/kZ+0RapxYsXh/3OO+8M+7hx48L+7rvvtuN0/7bMMsuEfddddw370KFDw97Q0BD2M844I+yTJk1qx+ngPzU3N4e9u2y72XbbbWt9BPjQGhvjPxdsbW3t4pNA93TwwQeH/Zvf/GbY11tvvbD37ds35TxPPfVU2N97772U69eaNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQKpuu11q0KBBYf/qV78a9mq1GvaiLVL77LNPR471/ynaTHDttdeGfeTIkaWu/9vf/jbs5557bqnrQC0dd9xxYV9uueVSrv/Rj3601OcfeuihsD/88MMZx4EOKdoiVfTzDerVWmutFfZDDz007KNGjUr53u233z7sWc/QggULwl60ver2228Pe9mNpfXKmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFW33S7Vr1+/sK+88sqlrlO01ea//uu/wn7EEUeEffTo0WEfPnx42JdffvmwF204KOrXXHNN2N9+++2wQ6Zll1027BtvvHHYv/e974V9jz32KPW9jY3xn48Ubd8p0tzcHPai5/yDDz4odX2A3qzo90C33npr2IcOHdqZx+l0f/rTn8J++eWXd/FJ6oM3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqm67XWrx4sVhf/XVV8O+yiqrhP2f//xn2Iu2OZVVtL1mwYIFYR88eHDYX3vttbD/7ne/69jBINC3b9+wb7755mG/8cYbw150H7/77rthL3pOHn744bDvvvvuYS/adlWkT5/4l8D99tsv7BdddFHYi349AuD/19DQUKpnydpMWGTPPfcM+6c//emw33HHHSnfW6+8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUnXb7VLz5s0L+z777BP23//+92FfccUVwz5z5sywT5o0Key/+MUvwv7GG2+E/frrrw970Vaeos9DR/Tr1y/sRVubbrrpplLX//73vx/2u+++O+wPPvhg2Iuez6LrDB8+vB2n+7eirXNnnXVW2GfPnh32W265JeyLFi0qdR5YkqzNODvssEPYJ0yYUPpMsCRTp04N+0477RT2Qw45JOx33nln2BcuXNihc7XXl770pbAfe+yxnfq9PYU3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqoZqtVpt60MLFiyoDBw4sCvO0+MUbfG47777wl60JeTrX/962MePH9+hc/U08+fPrwwYMKCmZ6jH56Rv375h/8EPfhD2k046qdT177jjjrAfeuihYS/aCle05en2228P+xZbbBH2xYsXh/3cc88Ne9E2qr333jvsRf74xz+G/Zxzzgn7m2++Wer6Tz31VKnPL0mtn5V6fE66iw8++CDs7fgx3i6bbrpp2KdNm5Zy/e6k1s9JpeJZqQdF//5ff/31UtfZa6+9wl70M7S7aOs58SYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEjVp9YH6On69+8f9qItUkVbQq6//vq0M9HzLLXUUmE//fTTwz527Niwv/3222H/5je/Gfai+7Joi9SWW24Z9gkTJoR98803D/uMGTPCfswxx4T9nnvuCXvRVoztttsu7AcffHDYR48eHfYpU6aEvcicOXPCvvbaa5e6Dj3TpZdeGvajjz465fpHHXVU2Iu2G0JPt9tuu9X6CN2aNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpTrZnXfeWesj0AsUbYUp2iL1zjvvhL1oS83kyZPDvs0224T9iCOOCPunP/3psBdtYfvBD34Q9okTJ4a9aDtTkQULFoT9D3/4Q6k+ZsyYsH/+858vdZ4TTjih1OfpXaZPn17rI9DL9e3bN+y77rpr2O++++6wv/vuu2lnylD0M+uiiy7q4pP0LN5kAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpGqrVarWtDy1YsKAycODArjhPj7PbbruF/fbbbw970X+OwYMHh/3VV1/t2MF6mPnz51cGDBhQ0zPU8jl56aWXwr7KKquEfdGiRWEv2l6z3HLLhX299dZrx+nadtppp4X9rLPOCvsHH3yQ8r29Ua2fFT9P8j333HNhX3fddUtdp7Ex/nPHoud85syZpa7fndT6OalUavusbL/99mH/zne+E/Zddtkl7GuvvXbYy24CLGvFFVcM+x577BH28ePHh72pqanU9xZtzRo9enTY77nnnlLXrzdtPSfeZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqfrU+gA93TrrrFPrI9ALvPzyy2Ev2i619NJLh32zzTYr9b1FW9Luv//+sN9yyy1hf+GFF8JuixS07emnnw572Z8/ra2tGcehB5gwYULYhw8fXuo6J598cthbWlpKn6mMom1XW2yxRdjbsWj1P9x7771h/+lPfxr27r5FqqO8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUtku1cn+9Kc/hb2xMZ7vbPegI3bYYYew77PPPmEv2rDxyiuvhP3nP/952N98882wL168OOxAvssvvzzse+21VxefBP7TMcccU+sjtEvRz77f/e53YT/++OPDvnDhwrQz9QTeZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqWyX6mRTp04N+4wZM8K+zjrrhH3dddcN+6uvvtqxg9GjtLS0hP3qq68u1YHuZ9q0aWF/5plnwr7RRht15nHoAQ4//PCwH3vssWH/whe+0ImnKTZz5sywv/POO2Ev2vhZtKGt6PdwtI83GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqoZqtVpt60MLFiyoDBw4sCvO02sUbW648sorw37fffeFvWjTQ9G2kZ5q/vz5lQEDBtT0DJ4TuoNaPyueE7qDWj8nlUp9PitLL7102It+T3PGGWeEfYUVVgj7LbfcEvYpU6aEfdKkSWF/+eWXw06utp4TbzIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFS2S9VI0d/Gv+GGG8I+atSosN90001hP+KII8L+9ttvt+N03Y9NINA+tX5WPCd0B7V+TioVzwr1z3YpAACgSxkyAACAVIYMAAAglSEDAABIZcgAAABS9an1AXqrBQsWhP2AAw4I+5lnnhn2Y445JuynnXZa2KdNm9b24QAA4EPwJgMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGW7VJ0p2jp17LHHluoAAFAr3mQAAACpDBkAAEAqQwYAAJDKkAEAAKRq15BRrVY7+xzwodTDPVoPZ4C21Po+rfX3Q3vUw31aD2eAJWnrHm3XkNHS0pJyGOgs9XCP1sMZoC21vk9r/f3QHvVwn9bDGWBJ2rpHG6rtGJVbW1srzc3NlaampkpDQ0Pa4eDDqlarlZaWlsqQIUMqjY21/b//PCfUs3p5Vjwn1LN6eU4qFc8K9au9z0m7hgwAAID28he/AQCAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACBVn/Z8qLW1tdLc3FxpamqqNDQ0dPaZoN2q1WqlpaWlMmTIkEpjY21nZs8J9axenhXPCfWsXp6TSsWzQv1q73PSriGjubm5ssYaa6QdDrLNmTOnsvrqq9f0DJ4TuoNaPyueE7qDWj8nlYpnhfrX1nPSrjG9qakp7UDQGerhHq2HM0Bban2f1vr7oT3q4T6thzPAkrR1j7ZryPCajnpXD/doPZwB2lLr+7TW3w/tUQ/3aT2cAZakrXvUX/wGAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASNWn1gcA6KnuuuuusDc0NIR955137szjUGc23njjsO+5555hP+qoo8L+2GOPhf0vf/lLqfNceOGFYV+8eHGp6wBUKt5kAAAAyQwZAABAKkMGAACQypABAACkMmQAAACpbJdK0rdv37Bvt912Yf/hD38Y9o9//ONpZwK6xo9//OOwFz3/V111VWcehzpz9NFHh/28884L+/LLL1/q+uuuu27YDzrooFLXKdpSdc8995S6DkCl4k0GAACQzJABAACkMmQAAACpDBkAAEAqQwYAAJDKdqkkAwcODHvRVo6XX3457B/5yEdKfR7oOmeffXbYv/KVr4T9vffeC/tdd92Vdibq329+85uw/+AHPwh72e1SWW666aawH3jggWGfPHlyZx4H6Oa8yQAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUtkuVSNFW6Rsl4L6tc0224S9b9++YX/ggQfCfsMNN6Sdifr3xhtvhP173/te2M8///ywL7vssmGfPXt22IcOHdqO0/3boEGDwr777ruH3XYp+HDWXHPNsPfv3z/sY8aMCfsxxxxT6ntvu+22sB9xxBGlrtMWbzIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFS2S9VIQ0NDrY8AXW6HHXYI+3e+852wF23SKNrWk6Xoe4cPHx72mTNnhn3s2LFpZ6LnufTSS8P+la98JeybbbZZ2BcsWJB2psiECRM69frQU4waNSrs++23X9iLftYMHDgw7NVqtWMH+1+KNiVm8yYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1SNFG0IWGaZZbr4JNB1Lr/88rCvv/76Yd94443D/sADD6SdKfLtb3877CuttFLYjzzyyLD/9a9/TTsTvccZZ5wR9qItbCNGjOjE01Qq/fr169TrQ7268sorw/7Rj3407FtttVXK97a0tIT92muvDftjjz0W9uuuuy7sCxcu7NjBSvImAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUndlyyy3D/sgjj3TxSSDfO++8E/ZabVsr2sqz5pprhr21tTXstsKR6be//W3Yi7aqTZ48OexFG3DKKtp29bnPfS7l+tBVijYEnnXWWWH/4he/GPY33ngj7E888UTYzz777LBPnTo17O+++27YZ8+eHfZ65U0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqkk77//ftjnz58f9oEDB4Z93XXXTTsT1Mrpp58e9qJtN88880zY//rXv6acZ7nllgv7uHHjwr7sssuGvWjLW9E2IOiIgw8+OOybbbZZ2IcPH96ZxyncagXdzXe/+92wf+lLXwr7+PHjw/6d73wn7G+99VbHDtZDeZMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKSyXSrJvHnzwv6nP/0p7HvuuWcnnga6xhprrBH2I488MuxFW9i+9rWvhf3VV1/t2MH+lwsuuCDs+++/f9ibm5vD/vGPfzzlPPQuG264YdhvvvnmsK+33nph79OnNj+yb7311pp8L/xfRRv/ijYEHnrooWH/+te/HvZ77rkn7HfeeWfYFy5cGHb+kzcZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAq26WANg0fPjzsRdtxVl555bCPHz8+7Pfdd1/HDva/jB07NuyHH354qeuceeaZCaeB/7HRRhuFfe211w57rbZIFTnhhBPCfuyxx3bxSeitTjnllLAXbZe64YYbwj558uSw2xbVObzJAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABS1dcKCyorrbRSrY9AL1C0veaQQw4J+89+9rOwNzbGf07R2toa9m233Tbs3/rWt8J+wQUXhH3FFVcM+/777x/2hoaGsF911VVhv+yyy8IOHVG0he3kk08O+znnnBP2ZZZZJu1MZQwePLgm3wv/V9HPiGq1Gvbrrrsu7LZIdS1vMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVLZL1ZnRo0fX+gj0AgcddFDYr7zyyrAXbfAo2iL1/PPPh33LLbcs1ffee++wr7baamEv2oLz6quvhv2LX/xi2KErXHzxxWGfMWNG2AcNGlTq+kVb5CZMmBD2AQMGlLo+dJU///nPYS/62VF0j7/77rthnzJlSscOxhJ5kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApLJdqpPdc889Yd9zzz27+CT0RgceeGDYJ06cGPb33nsv7PPmzQv75z//+bC/+eabYT///PPDvuOOO4a9aHNIQ0ND2Iu2YK288sphnzNnTth32mmnsM+cOTPskOmOO+5IuU7Rc7LeeuuF/dRTTw37iBEjwr7mmmuGfdasWW0fjl5h6623Dvtf/vKXsC9evDjsn/70p8N+3HHHhf273/1u2H/729+Gveic06dPDzvt400GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqlONnv27FKf79u3b9ht8aAjjj766LAX3ZdnnHFG2Iu2UZV17LHHhv2yyy4L+7bbbpvyvUVbdoq2v9kiRU/Qr1+/sBdtkSpStHXugw8+KH0murfBgweH/fe//33Yhw4dGvYTTjgh7Ndcc03Y33jjjbBPmDAh7EXbpZZffvmwr7jiimHnw/EmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZbtUJ3v//fdLfb5oC87SSy+dcRx6mUmTJoX9pptuCvucOXM68ziVlVdeOezDhw8vdZ0xY8aEferUqaWuM3fu3FKfh+6kaFtcWT/72c/C7vnpfZ588smwDxgwIOzjxo0Le9EWqbKOP/74Up//4x//GPayPztoH28yAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUDdVqtdrWhxYsWFAZOHBgV5yn15g2bVrYN9xww7BfeumlYf/qV7+adqbubP78+YXbLbqK5+Tfiv49FG27KbqPZ86cGfZhw4Z17GDU/Fmpx+dkpZVWCvvEiRPDft1115XqnW3w4MFhnz59etjL/vdfd911w/6Pf/yj1HW6k1o/J5VKfT4r3/rWt8J+yimnhL1///4p3ztjxoywr7/++mGfNWtW2D/72c+GvWhrFkvW1nPiTQYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkKpPrQ/QW02ePDnsq622Wti/8Y1vdOZxIFXRtqhjjjkm7K+88krYd95557QzQZGLL7447HvttVfYi7abNTc3h/3FF18M+/PPPx/2kSNHlvrek08+OexltyOdf/75YS/656L3Oeuss8L+3nvvhX3zzTcP+6hRo0p97worrBD22267Lexjx44Ne9EzR+fwJgMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGW7VJ2pVqthX7x4cRefBNq25pprhv3LX/5y2Ivu78svvzzsc+fO7djBoITx48eHfe211w77tttuG/Z777037C+88ELYp02bFvZPfOITYW9qagp7kaLnbfr06WH/3ve+F/aFCxeW+l56n/POO6/WR6AOeZMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKSyXarODBgwIOx777132G+++ebOPA4s0ZQpU8JetHXqmmuuCXvRVhvoCo888kjYH3744bBfffXVYb/kkkvCvtZaa5XqWd58882wb7zxxp36vQCVijcZAABAMkMGAACQypABAACkMmQAAACpDBkAAEAq26Vq5IADDgj7okWLwv7MM8905nGgQyZOnBj2008/PeyTJk3qzONAqhNPPDHsSy+9dNiXX375UtfffPPNwz5mzJhS15k/f37Yd9lll1LXAcjkTQYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkKqhWq1W2/rQggULKgMHDuyK8/Qa119/fdg32mijsI8ePTrss2bNSjtTdzZ//vzKgAEDanoGzwndQa2fFc8J3UGtn5NKxbNC/WvrOfEmAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABI1afWB+itDjrooFofAQAAOoU3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApDJkAAAAqdo1ZFSr1c4+B3wo9XCP1sMZoC21vk9r/f3QHvVwn9bDGWBJ2rpH2zVktLS0pBwGOks93KP1cAZoS63v01p/P7RHPdyn9XAGWJK27tGGajtG5dbW1kpzc3Olqamp0tDQkHY4+LCq1WqlpaWlMmTIkEpjY23/7z/PCfWsXp4Vzwn1rF6ek0rFs0L9au9z0q4hAwAAoL38xW8AACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJUhAwAASNWnPR9qbW2tNDc3V5qamioNDQ2dfSZot2q1WmlpaakMGTKk0thY25nZc0I9q5dnxXNCPauX56RS8axQv9r7nLRryGhubq6sscYaaYeDbHPmzKmsvvrqNT2D54TuoNbPiueE7qDWz0ml4lmh/rX1nLRrTG9qako7EHSGerhH6+EM0JZa36e1/n5oj3q4T+vhDLAkbd2j7RoyvKaj3tXDPVoPZ4C21Po+rfX3Q3vUw31aD2eAJWnrHvUXvwEAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVIYMAAAglSEDAABIZcgAAABS9an1AQCyDBs2LOx/+MMfwr7UUkuFfc0110w7EwD0Rt5kAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpbJcCup3x48eH/cADDwz7iiuuGPbf//73aWcCAP7NmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIJXtUkDNrbrqqmG/6aabwr7NNtuEvVqthn3q1Klh/9KXvtSO0wEAZXmTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACk6vLtUssvv3zYDzzwwLAvXLgw7CNHjgx7U1NT2A8++OCw33vvvWF/8cUXw57l5ZdfDvukSZPC/vjjj3fmcaBLDBs2LOznnXde2LfeeutS1//Wt74V9qLn5/XXXy91fcjU0NAQ9uuuuy7se+yxR9g33njjsM+dO7djBwNI4E0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJCqy7dLnXrqqWEfO3ZsF5/kf+y+++41+d4iRdtxpk2bFvaiLSRF/YUXXujQuSDDiiuuGPairTllFW3Tueeee1KuD5n69+8f9o9//ONhL9rOWPRz7Morr+zYwQASeJMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKTq8u1S++23X6de//XXXw/73/72t0793meffTbsG2ywQdgHDRoU9s033zzsw4cPD/uZZ54Z9qJ/Xtul6ArDhg0L+69+9auwNzQ0lLp+0a8jkyZNKnUdqKV33nkn7DNmzAj7aqutFvZVVlkl7UzQG5144olh79evX9g32mijsB988MGlvnf69Olh32STTUpdp155kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApOry7VK77bZb2Iu20Tz33HOlrl+0reOll14qdZ3O1tTUFPa///3vYR86dGip648ePTrst912W6nrQEcceuihYS+6j2+//fawf+UrXwn7iy++2LGDQTfwk5/8JOw77bRT2Is23UBPt+OOO4a9aCNn0ef33XffsJfdfFitVkt9fv311w/7tGnTwr7xxhuXun6teZMBAACkMmQAAACpDBkAAEAqQwYAAJDKkAEAAKTq8u1SM2fOLNV7qj333DPsZbdILVq0KOxXXHFF6TNBWQ899FDYR4wYEfYXXngh7CeccELYbZGiN/rzn/9c6vMHHHBA2MeNGxf2etu2SM81ePDgsF933XVhX2eddUpdf+DAgWFfbrnlwl60LeqJJ54I+xZbbFHqPGU1NsZ/1l90/u7GmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWXb5fqqfr16xf2iy++OOyHHXZYyvduu+22YX/qqadSrg+VSqWy9957h33rrbcOe7VaDftvfvObsC9cuLBjB4NepGgzTtHPn9GjR4f9sssuSzsTVCqVyqhRo8JetOlyjTXW6MzjFNp4443D/tprr4V95ZVXDvuQIUPCPnHixLCvvvrq7Tjdv02bNq3U5+uVNxkAAEAqQwYAAJDKkAEAAKQyZAAAAKkMGQAAQCrbpUr65Cc/GfZDDz007Icffnip67/33nthP+6448I+ffr0UteHJRk0aFDYP/GJT6Rc/8033wz73LlzU65f5Pjjjw972Q0nY8eOzTgOdEjR1rYiRVunINvJJ58c9qwtUosWLQr7uHHjwv7II4+E/dlnny31va+//nrYi36mlN0i9cILL4S96PeU3Y03GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAKtulCnzsYx8L++TJk8O+1FJLpXxv0faQ2bNnh/2DDz5I+V6oVIrvp5EjR4a9sTH+c4rW1taw33///R072P9ywgknlPr8scceG/Y111yz1HVOPPHEsBdtFHnxxRdLXR+gnu26665h32abbVKuX/R7naJtSw8++GDK95ZVdotUkUmTJoX9tddeS7l+rXmTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACksl2qwAEHHBD2rC1SRfr16xf22267LeyPP/542H/3u9+F/eabbw771KlT23E6erodd9wx7J/4xCfCXrRFqmhDSNmNGSNGjCh1ntGjR5e6/ttvvx32uXPnhn2DDTYI+29/+9uwH3TQQWGfNWtWO04HUF+KNuwtu+yypa7z0EMPhf373/9+2Dt7i9QKK6wQ9t133z3sO+ywQ6nrF/3z3n777aWu0914kwEAAKQyZAAAAKkMGQAAQCpDBgAAkMqQAQAApLJdqsBNN90U9o022ijsW221VdhXXnnltDNFttxyy1L9e9/7XtgvvPDCsJ977rlhf+WVV9o+HHWrqakp7GuvvXap6zQ3N4f96quvDvvzzz8f9mHDhoX9pJNOCvvee+8d9qLtVZMnTw77+eefH/aBAweG/e677y71ecjU0NAQ9mq12sUnobe6/PLLw170e5358+eH/fOf/3zYX3755Y4d7EP6yle+EvbTTz+91HWefvrpsBdtLK3VP29X8SYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1SBhx56KOyf+cxnwj506NCwF21cWHXVVcO+3377hf2LX/xi2Iu2jRRpbIznym984xthHzlyZNg/9alPhb21tbXUeaiN7bffPuw//vGPS13niiuuCPsPfvCDsBfd9+edd17Y99hjj7C3tLSE/YYbbgj72LFjw77++uuH/dJLLy31vXfddVfYZ82aFXboCFukqLUbb7yxVK83e+21V9hPPfXUUtd5//33w170s6Onb5Eq4k0GAACQypABAACkMmQAAACpDBkAAEAqQwYAAJDKdqkks2fPLtWL3HHHHWG/9957w37ssceG/WMf+1ip7y2y4447hr1oW8+5556b8r10rk033TTlOkVbpIrcdNNNYd96661LXWfvvfcO+3333Rf2bbbZJuwPPPBAqe+98MILw170PEAt/e1vf6v1EaCu3HLLLWEvu7ntuOOOC/vll19e9kg9mjcZAABAKkMGAACQypABAACkMmQAAACpDBkAAEAq26W6iWuvvTbsv/71r8P+xz/+Mew77LBDynnWW2+9lOtQG4MGDQp7Q0ND2CdNmlTq+iNGjAj7WmutVep7TzzxxLAXbZEaNmxY2H/1q1+lfG/RdimoRzNnzqz1EaAmfvjDH4a9sTH+s/XW1tZS1y/6GcR/8iYDAABIZcgAAABSGTIAAIBUhgwAACCVIQMAAEhlu1Q39/7774f9iSeeCHvWdqnnnnsu5TrUl2q1WqqXVbTBo+j6m266adhnz54d9mWWWSbs//znP8P+iU98Iuzz588POwD1o1+/fmHffPPNw172Z9Dxxx8f9hkzZrTjdHiTAQAApDJkAAAAqQwZAABAKkMGAACQypABAACk6jXbpQYPHhz2I488MuzTp08P+w033JB2pgxLLbVU2DfbbLOU6xdtr3rkkUdSrk9tTJo0KewnnXRS2Pfee++wb7PNNmEfMWJE2Juamto+3P/jsMMOC3tDQ0PYX3vttbCfdtppYX/xxRdLnQe6k6WXXrrWR4AUyy67bNgPOeSQsO+yyy6lrn/dddeF/dprrw170ZYq/pM3GQAAQCpDBgAAkMqQAQAApDJkAAAAqQwZAABAqh63XeojH/lI2P/whz+E/aMf/WjYV1hhhbQzZVh11VXD/o1vfCPsO++8c8r3PvPMM2F/4IEHUq5Pbbz33nthf+edd8JetNnjwQcfDHu1Wu3YwdqppaUl7EXb3+64447OPA7UpT322CPs48eP7+KTQPsUbSC84oorwv65z32u1PVPOOGEsE+YMCHstkh9ON5kAAAAqQwZAABAKkMGAACQypABAACkMmQAAACpetx2qQsvvDDsRVukiqy99tphf/bZZ8P+7rvvlrp+//79w37yySeHvWiLVNEmhiINDQ1hL9rWc9xxx5W6Pt3DE088EfYxY8aEvej+22mnnVLO88tf/jLsf//738P+l7/8Jez33Xdfynmglv71r3+F/emnnw77Jpts0pnHgS6z2mqrhb3sFqmZM2eG/eKLLy59JjrOmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWP2y511113hf2AAw4odZ0nn3wy7EVbbebPn1/q+gMHDgz75ptvXuo6ZRVtkdp3333DbltP73LbbbeV6kC+xYsXh33hwoWlrrPLLruEffz48aXPBJk23HDDsJ944omlrvPcc8+F/dOf/nTpM5HPmwwAACCVIQMAAEhlyAAAAFIZMgAAgFSGDAAAIFWP2y41ZcqUsF9//fVhP+igg0pdv7O3P5X1/vvvh/3CCy8M+4033hj2Rx99NOtIAHSCp556KuwjR44M+/LLL9+Jp4GO++53vxv2Aw88sNR1ijalzZo1q/SZyOdNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqsdtl3rhhRfCfsQRR4T91ltvDfvOO+8c9ueeey7so0ePbvtw/4/p06eX+vzdd99d6jpFW0gA6J7OPPPMsA8fPjzsN9xwQ2ceB9q0ySabhH3AgAGlrnP55ZeHvej3RtQHbzIAAIBUhgwAACCVIQMAAEhlyAAAAFIZMgAAgFQN1Wq12taHFixYUBk4cGBXnAc6ZP78+aW3VWTznNAd1PpZ8ZzQHdT6OalUesazcs4554T9xBNPDPusWbPCvscee4T92Wef7djBSNHWc+JNBgAAkMqQAQAApDJkAAAAqQwZAABAKkMGAACQqk+tDwAAQM8zefLksBdtl/rGN74RdlukuidvMgAAgFSGDAAAIJUhAwAASGXIAAAAUhkyAACAVLZLAQCQ7q677gp7nz5++9kbeJMBAACkMmQAAACpDBkAAEAqQwYAAJCqXUNGtVrt7HPAh1IP92g9nAHaUuv7tNbfD+1RD/dpPZwBlqSte7RdQ0ZLS0vKYaCz1MM9Wg9ngLbU+j6t9fdDe9TDfVoPZ4Alaesebai2Y1RubW2tNDc3V5qamioNDQ1ph4MPq1qtVlpaWipDhgypNDbW9v/+85xQz+rlWfGcUM/q5TmpVDwr1K/2PiftGjIAAADay1/8BgAAUhkyAACAVIYMAAAglSEDAABIZcgAAABSGTIAAIBUhgwAACDV/wGPUF/H+0rWkgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Visualize the first 16 elements of the training set\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "for i in range(16):\n",
        "    ax = fig.add_subplot(4, 4, i + 1)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n",
        "    ax.imshow(dataset1.data[i], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYBp0zPEoCkj"
      },
      "source": [
        "# Implementations (7 marks)\n",
        "In PyTorch, optimisers are created by passing the model parameters and hyperparameters to the optimiser class. The optimiser is then used to update the model parameters after each forward pass. Following this convension, we will write our optimisers as *classes*, an example structure for which is given below. Feel free to add whatever parameters or class functions that you see fit.\n",
        "\n",
        "Implement the optimisers **AdaGrad** (Adaptive gradient), **RMSProp** (Root mean square propagation), and **Adam** (Adaptive moments) below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WQbpl-ny0zCX"
      },
      "outputs": [],
      "source": [
        "class adagrad_optimizer():\n",
        "    def __init__(self, model, lr=0.001, epsilon=1e-6):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.epsilon = epsilon\n",
        "        self.squared_gradients = [torch.zeros_like(p) for p in model.parameters()]\n",
        "\n",
        "    # The Adagrad optimizer adjusts the learning rate based on the cumulative sum of squared gradients for each parameter. This helps reduce the learning rate for parameters with frequently occurring features, while parameters associated with infrequent features get larger updates.\n",
        "    def step(self):\n",
        "        for i, param in enumerate(self.model.parameters()):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "\n",
        "            self.squared_gradients[i] += param.grad.data ** 2 # Accumulating  the squared gradients\n",
        "\n",
        "            adjusted_lr = self.lr / (self.squared_gradients[i].sqrt() + self.epsilon)  # Updating the parameters\n",
        "            param.data -= adjusted_lr * param.grad.data\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.model.parameters():\n",
        "            if param.grad is not None:\n",
        "                param.grad.detach_()\n",
        "                param.grad.zero_()\n",
        "\n",
        "class rmsprop_optimizer():\n",
        "    def __init__(self, model, lr=0.001, beta=0.99, epsilon=1e-6):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.beta = beta\n",
        "        self.epsilon = epsilon\n",
        "        self.squared_avg = [torch.zeros_like(p) for p in model.parameters()]\n",
        "\n",
        "        #RMSprop addresses some issues with Adagrad by using an exponentially decaying average of squared gradients rather than accumulating all past squared gradients. This results in more stable updates and mitigates the excessive decay of learning rates seen in Adagrad.\n",
        "    def step(self):\n",
        "        for i, param in enumerate(self.model.parameters()):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "\n",
        "            self.squared_avg[i] = self.beta * self.squared_avg[i] + (1 - self.beta) * param.grad.data ** 2 # Updating  running average of squared gradients\n",
        "\n",
        "            adjusted_lr = self.lr / (self.squared_avg[i].sqrt() + self.epsilon) # Updating parameters\n",
        "            param.data -= adjusted_lr * param.grad.data\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.model.parameters():\n",
        "            if param.grad is not None:\n",
        "                param.grad.detach_()\n",
        "                param.grad.zero_()\n",
        "\n",
        "class adam_optimizer():\n",
        "    def __init__(self, model, lr=0.001, beta1=0.9, beta2=0.99, epsilon=1e-6):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.m = [torch.zeros_like(p) for p in model.parameters()]  # First moment vector\n",
        "        self.v = [torch.zeros_like(p) for p in model.parameters()]  # Second moment vector\n",
        "        self.t = 0  # Time step\n",
        "\n",
        "        # The Adam optimizer combines concepts from both RMSprop and momentum. It uses two moving averages: one for the gradient (first moment) and another for the squared gradient (second moment). It also corrects for the initialization bias in these averages by computing bias-corrected estimates.\n",
        "\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        for i, param in enumerate(self.model.parameters()):\n",
        "            if param.grad is None:\n",
        "                continue\n",
        "\n",
        "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * param.grad.data # Update biased first moment estimate\n",
        "\n",
        "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * param.grad.data ** 2 # Update biased second raw moment estimate\n",
        "\n",
        "            m_hat = self.m[i] / (1 - self.beta1 ** self.t) # Correct bias in first and second moment\n",
        "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "            # Update parameters\n",
        "            param.data -= self.lr * m_hat / (v_hat.sqrt() + self.epsilon)\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for param in self.model.parameters():\n",
        "            if param.grad is not None:\n",
        "                param.grad.detach_()\n",
        "                param.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mCSP_SEcmi7L"
      },
      "outputs": [],
      "source": [
        "def train(log_interval, model, device, train_loader, test_loader, optimizer):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "        output = model(data)   # Forward pass\n",
        "        loss = F.nll_loss(output, target)  # Compute loss\n",
        "        loss.backward()  # Backpropagation to compute gradients\n",
        "\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f'Train: [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "            test(model, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id8H08tloPTr"
      },
      "source": [
        "# Model training and submission (3 marks)\n",
        "\n",
        "Use the following training settings for your model training. You may have to run this code multiple times, or create a loop, to train the model with different optimisers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTPITptmmv09",
        "outputId": "125619bf-fc67-4d8a-f977-2ef9b2655f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: [0/60000 (0%)]\tLoss: 2.302889\n",
            "\n",
            "Test set: Average loss: 2.2501, Accuracy: 2252/10000 (23%)\n",
            "\n",
            "Train: [640/60000 (1%)]\tLoss: 1.003245\n",
            "\n",
            "Test set: Average loss: 1.0554, Accuracy: 6924/10000 (69%)\n",
            "\n",
            "Train: [1280/60000 (2%)]\tLoss: 0.774821\n",
            "\n",
            "Test set: Average loss: 0.6260, Accuracy: 8484/10000 (85%)\n",
            "\n",
            "Train: [1920/60000 (3%)]\tLoss: 0.540468\n",
            "\n",
            "Test set: Average loss: 0.5246, Accuracy: 8587/10000 (86%)\n",
            "\n",
            "Train: [2560/60000 (4%)]\tLoss: 0.375651\n",
            "\n",
            "Test set: Average loss: 0.4599, Accuracy: 8662/10000 (87%)\n",
            "\n",
            "Train: [3200/60000 (5%)]\tLoss: 0.501221\n",
            "\n",
            "Test set: Average loss: 0.4289, Accuracy: 8826/10000 (88%)\n",
            "\n",
            "Train: [3840/60000 (6%)]\tLoss: 0.545869\n",
            "\n",
            "Test set: Average loss: 0.4046, Accuracy: 8900/10000 (89%)\n",
            "\n",
            "Train: [4480/60000 (7%)]\tLoss: 0.347474\n",
            "\n",
            "Test set: Average loss: 0.3909, Accuracy: 8885/10000 (89%)\n",
            "\n",
            "Train: [5120/60000 (9%)]\tLoss: 0.384817\n",
            "\n",
            "Test set: Average loss: 0.3384, Accuracy: 9082/10000 (91%)\n",
            "\n",
            "Train: [5760/60000 (10%)]\tLoss: 0.235890\n",
            "\n",
            "Test set: Average loss: 0.3240, Accuracy: 9102/10000 (91%)\n",
            "\n",
            "Train: [6400/60000 (11%)]\tLoss: 0.210070\n",
            "\n",
            "Test set: Average loss: 0.3190, Accuracy: 9115/10000 (91%)\n",
            "\n",
            "Train: [7040/60000 (12%)]\tLoss: 0.257708\n",
            "\n",
            "Test set: Average loss: 0.3121, Accuracy: 9109/10000 (91%)\n",
            "\n",
            "Train: [7680/60000 (13%)]\tLoss: 0.387247\n",
            "\n",
            "Test set: Average loss: 0.3006, Accuracy: 9184/10000 (92%)\n",
            "\n",
            "Train: [8320/60000 (14%)]\tLoss: 0.202992\n",
            "\n",
            "Test set: Average loss: 0.3066, Accuracy: 9131/10000 (91%)\n",
            "\n",
            "Train: [8960/60000 (15%)]\tLoss: 0.312909\n",
            "\n",
            "Test set: Average loss: 0.2793, Accuracy: 9222/10000 (92%)\n",
            "\n",
            "Train: [9600/60000 (16%)]\tLoss: 0.383991\n",
            "\n",
            "Test set: Average loss: 0.2740, Accuracy: 9230/10000 (92%)\n",
            "\n",
            "Train: [10240/60000 (17%)]\tLoss: 0.228089\n",
            "\n",
            "Test set: Average loss: 0.2713, Accuracy: 9242/10000 (92%)\n",
            "\n",
            "Train: [10880/60000 (18%)]\tLoss: 0.488892\n",
            "\n",
            "Test set: Average loss: 0.2671, Accuracy: 9232/10000 (92%)\n",
            "\n",
            "Train: [11520/60000 (19%)]\tLoss: 0.179585\n",
            "\n",
            "Test set: Average loss: 0.2576, Accuracy: 9277/10000 (93%)\n",
            "\n",
            "Train: [12160/60000 (20%)]\tLoss: 0.230884\n",
            "\n",
            "Test set: Average loss: 0.2661, Accuracy: 9263/10000 (93%)\n",
            "\n",
            "Train: [12800/60000 (21%)]\tLoss: 0.234140\n",
            "\n",
            "Test set: Average loss: 0.2621, Accuracy: 9249/10000 (92%)\n",
            "\n",
            "Train: [13440/60000 (22%)]\tLoss: 0.161808\n",
            "\n",
            "Test set: Average loss: 0.2484, Accuracy: 9286/10000 (93%)\n",
            "\n",
            "Train: [14080/60000 (23%)]\tLoss: 0.354302\n",
            "\n",
            "Test set: Average loss: 0.2483, Accuracy: 9273/10000 (93%)\n",
            "\n",
            "Train: [14720/60000 (25%)]\tLoss: 0.100471\n",
            "\n",
            "Test set: Average loss: 0.2452, Accuracy: 9311/10000 (93%)\n",
            "\n",
            "Train: [15360/60000 (26%)]\tLoss: 0.316865\n",
            "\n",
            "Test set: Average loss: 0.2402, Accuracy: 9314/10000 (93%)\n",
            "\n",
            "Train: [16000/60000 (27%)]\tLoss: 0.416070\n",
            "\n",
            "Test set: Average loss: 0.2394, Accuracy: 9341/10000 (93%)\n",
            "\n",
            "Train: [16640/60000 (28%)]\tLoss: 0.125383\n",
            "\n",
            "Test set: Average loss: 0.2242, Accuracy: 9380/10000 (94%)\n",
            "\n",
            "Train: [17280/60000 (29%)]\tLoss: 0.393715\n",
            "\n",
            "Test set: Average loss: 0.2265, Accuracy: 9360/10000 (94%)\n",
            "\n",
            "Train: [17920/60000 (30%)]\tLoss: 0.242477\n",
            "\n",
            "Test set: Average loss: 0.2270, Accuracy: 9360/10000 (94%)\n",
            "\n",
            "Train: [18560/60000 (31%)]\tLoss: 0.259881\n",
            "\n",
            "Test set: Average loss: 0.2194, Accuracy: 9389/10000 (94%)\n",
            "\n",
            "Train: [19200/60000 (32%)]\tLoss: 0.176476\n",
            "\n",
            "Test set: Average loss: 0.2215, Accuracy: 9346/10000 (93%)\n",
            "\n",
            "Train: [19840/60000 (33%)]\tLoss: 0.348332\n",
            "\n",
            "Test set: Average loss: 0.2170, Accuracy: 9375/10000 (94%)\n",
            "\n",
            "Train: [20480/60000 (34%)]\tLoss: 0.179956\n",
            "\n",
            "Test set: Average loss: 0.2103, Accuracy: 9400/10000 (94%)\n",
            "\n",
            "Train: [21120/60000 (35%)]\tLoss: 0.300512\n",
            "\n",
            "Test set: Average loss: 0.2124, Accuracy: 9396/10000 (94%)\n",
            "\n",
            "Train: [21760/60000 (36%)]\tLoss: 0.214251\n",
            "\n",
            "Test set: Average loss: 0.2095, Accuracy: 9400/10000 (94%)\n",
            "\n",
            "Train: [22400/60000 (37%)]\tLoss: 0.338667\n",
            "\n",
            "Test set: Average loss: 0.2036, Accuracy: 9431/10000 (94%)\n",
            "\n",
            "Train: [23040/60000 (38%)]\tLoss: 0.205185\n",
            "\n",
            "Test set: Average loss: 0.2057, Accuracy: 9430/10000 (94%)\n",
            "\n",
            "Train: [23680/60000 (39%)]\tLoss: 0.202116\n",
            "\n",
            "Test set: Average loss: 0.1992, Accuracy: 9451/10000 (95%)\n",
            "\n",
            "Train: [24320/60000 (41%)]\tLoss: 0.335845\n",
            "\n",
            "Test set: Average loss: 0.2022, Accuracy: 9438/10000 (94%)\n",
            "\n",
            "Train: [24960/60000 (42%)]\tLoss: 0.198897\n",
            "\n",
            "Test set: Average loss: 0.2006, Accuracy: 9439/10000 (94%)\n",
            "\n",
            "Train: [25600/60000 (43%)]\tLoss: 0.322475\n",
            "\n",
            "Test set: Average loss: 0.1920, Accuracy: 9453/10000 (95%)\n",
            "\n",
            "Train: [26240/60000 (44%)]\tLoss: 0.360256\n",
            "\n",
            "Test set: Average loss: 0.1909, Accuracy: 9473/10000 (95%)\n",
            "\n",
            "Train: [26880/60000 (45%)]\tLoss: 0.161936\n",
            "\n",
            "Test set: Average loss: 0.2161, Accuracy: 9351/10000 (94%)\n",
            "\n",
            "Train: [27520/60000 (46%)]\tLoss: 0.241029\n",
            "\n",
            "Test set: Average loss: 0.1924, Accuracy: 9462/10000 (95%)\n",
            "\n",
            "Train: [28160/60000 (47%)]\tLoss: 0.344789\n",
            "\n",
            "Test set: Average loss: 0.1885, Accuracy: 9463/10000 (95%)\n",
            "\n",
            "Train: [28800/60000 (48%)]\tLoss: 0.228831\n",
            "\n",
            "Test set: Average loss: 0.1927, Accuracy: 9438/10000 (94%)\n",
            "\n",
            "Train: [29440/60000 (49%)]\tLoss: 0.241357\n",
            "\n",
            "Test set: Average loss: 0.1875, Accuracy: 9463/10000 (95%)\n",
            "\n",
            "Train: [30080/60000 (50%)]\tLoss: 0.161624\n",
            "\n",
            "Test set: Average loss: 0.1838, Accuracy: 9461/10000 (95%)\n",
            "\n",
            "Train: [30720/60000 (51%)]\tLoss: 0.255806\n",
            "\n",
            "Test set: Average loss: 0.1846, Accuracy: 9479/10000 (95%)\n",
            "\n",
            "Train: [31360/60000 (52%)]\tLoss: 0.265814\n",
            "\n",
            "Test set: Average loss: 0.1825, Accuracy: 9485/10000 (95%)\n",
            "\n",
            "Train: [32000/60000 (53%)]\tLoss: 0.264083\n",
            "\n",
            "Test set: Average loss: 0.1792, Accuracy: 9487/10000 (95%)\n",
            "\n",
            "Train: [32640/60000 (54%)]\tLoss: 0.416135\n",
            "\n",
            "Test set: Average loss: 0.1808, Accuracy: 9481/10000 (95%)\n",
            "\n",
            "Train: [33280/60000 (55%)]\tLoss: 0.170391\n",
            "\n",
            "Test set: Average loss: 0.1754, Accuracy: 9496/10000 (95%)\n",
            "\n",
            "Train: [33920/60000 (57%)]\tLoss: 0.248315\n",
            "\n",
            "Test set: Average loss: 0.1763, Accuracy: 9493/10000 (95%)\n",
            "\n",
            "Train: [34560/60000 (58%)]\tLoss: 0.236911\n",
            "\n",
            "Test set: Average loss: 0.1752, Accuracy: 9505/10000 (95%)\n",
            "\n",
            "Train: [35200/60000 (59%)]\tLoss: 0.373585\n",
            "\n",
            "Test set: Average loss: 0.1731, Accuracy: 9500/10000 (95%)\n",
            "\n",
            "Train: [35840/60000 (60%)]\tLoss: 0.099115\n",
            "\n",
            "Test set: Average loss: 0.1715, Accuracy: 9510/10000 (95%)\n",
            "\n",
            "Train: [36480/60000 (61%)]\tLoss: 0.200393\n",
            "\n",
            "Test set: Average loss: 0.1686, Accuracy: 9513/10000 (95%)\n",
            "\n",
            "Train: [37120/60000 (62%)]\tLoss: 0.126530\n",
            "\n",
            "Test set: Average loss: 0.1690, Accuracy: 9512/10000 (95%)\n",
            "\n",
            "Train: [37760/60000 (63%)]\tLoss: 0.113586\n",
            "\n",
            "Test set: Average loss: 0.1702, Accuracy: 9521/10000 (95%)\n",
            "\n",
            "Train: [38400/60000 (64%)]\tLoss: 0.202358\n",
            "\n",
            "Test set: Average loss: 0.1656, Accuracy: 9522/10000 (95%)\n",
            "\n",
            "Train: [39040/60000 (65%)]\tLoss: 0.095491\n",
            "\n",
            "Test set: Average loss: 0.1645, Accuracy: 9528/10000 (95%)\n",
            "\n",
            "Train: [39680/60000 (66%)]\tLoss: 0.170055\n",
            "\n",
            "Test set: Average loss: 0.1672, Accuracy: 9512/10000 (95%)\n",
            "\n",
            "Train: [40320/60000 (67%)]\tLoss: 0.127530\n",
            "\n",
            "Test set: Average loss: 0.1627, Accuracy: 9528/10000 (95%)\n",
            "\n",
            "Train: [40960/60000 (68%)]\tLoss: 0.088082\n",
            "\n",
            "Test set: Average loss: 0.1631, Accuracy: 9540/10000 (95%)\n",
            "\n",
            "Train: [41600/60000 (69%)]\tLoss: 0.142889\n",
            "\n",
            "Test set: Average loss: 0.1594, Accuracy: 9540/10000 (95%)\n",
            "\n",
            "Train: [42240/60000 (70%)]\tLoss: 0.051286\n",
            "\n",
            "Test set: Average loss: 0.1574, Accuracy: 9561/10000 (96%)\n",
            "\n",
            "Train: [42880/60000 (71%)]\tLoss: 0.182325\n",
            "\n",
            "Test set: Average loss: 0.1575, Accuracy: 9542/10000 (95%)\n",
            "\n",
            "Train: [43520/60000 (72%)]\tLoss: 0.307967\n",
            "\n",
            "Test set: Average loss: 0.1569, Accuracy: 9554/10000 (96%)\n",
            "\n",
            "Train: [44160/60000 (74%)]\tLoss: 0.086139\n",
            "\n",
            "Test set: Average loss: 0.1568, Accuracy: 9559/10000 (96%)\n",
            "\n",
            "Train: [44800/60000 (75%)]\tLoss: 0.112760\n",
            "\n",
            "Test set: Average loss: 0.1533, Accuracy: 9554/10000 (96%)\n",
            "\n",
            "Train: [45440/60000 (76%)]\tLoss: 0.271004\n",
            "\n",
            "Test set: Average loss: 0.1548, Accuracy: 9550/10000 (96%)\n",
            "\n",
            "Train: [46080/60000 (77%)]\tLoss: 0.306274\n",
            "\n",
            "Test set: Average loss: 0.1547, Accuracy: 9555/10000 (96%)\n",
            "\n",
            "Train: [46720/60000 (78%)]\tLoss: 0.122926\n",
            "\n",
            "Test set: Average loss: 0.1525, Accuracy: 9555/10000 (96%)\n",
            "\n",
            "Train: [47360/60000 (79%)]\tLoss: 0.086974\n",
            "\n",
            "Test set: Average loss: 0.1510, Accuracy: 9569/10000 (96%)\n",
            "\n",
            "Train: [48000/60000 (80%)]\tLoss: 0.120300\n",
            "\n",
            "Test set: Average loss: 0.1555, Accuracy: 9556/10000 (96%)\n",
            "\n",
            "Train: [48640/60000 (81%)]\tLoss: 0.262580\n",
            "\n",
            "Test set: Average loss: 0.1496, Accuracy: 9569/10000 (96%)\n",
            "\n",
            "Train: [49280/60000 (82%)]\tLoss: 0.258617\n",
            "\n",
            "Test set: Average loss: 0.1502, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train: [49920/60000 (83%)]\tLoss: 0.286172\n",
            "\n",
            "Test set: Average loss: 0.1563, Accuracy: 9565/10000 (96%)\n",
            "\n",
            "Train: [50560/60000 (84%)]\tLoss: 0.165597\n",
            "\n",
            "Test set: Average loss: 0.1488, Accuracy: 9587/10000 (96%)\n",
            "\n",
            "Train: [51200/60000 (85%)]\tLoss: 0.264632\n",
            "\n",
            "Test set: Average loss: 0.1490, Accuracy: 9589/10000 (96%)\n",
            "\n",
            "Train: [51840/60000 (86%)]\tLoss: 0.237683\n",
            "\n",
            "Test set: Average loss: 0.1478, Accuracy: 9580/10000 (96%)\n",
            "\n",
            "Train: [52480/60000 (87%)]\tLoss: 0.097842\n",
            "\n",
            "Test set: Average loss: 0.1470, Accuracy: 9595/10000 (96%)\n",
            "\n",
            "Train: [53120/60000 (88%)]\tLoss: 0.144686\n",
            "\n",
            "Test set: Average loss: 0.1469, Accuracy: 9591/10000 (96%)\n",
            "\n",
            "Train: [53760/60000 (90%)]\tLoss: 0.092906\n",
            "\n",
            "Test set: Average loss: 0.1454, Accuracy: 9587/10000 (96%)\n",
            "\n",
            "Train: [54400/60000 (91%)]\tLoss: 0.181292\n",
            "\n",
            "Test set: Average loss: 0.1487, Accuracy: 9582/10000 (96%)\n",
            "\n",
            "Train: [55040/60000 (92%)]\tLoss: 0.211558\n",
            "\n",
            "Test set: Average loss: 0.1461, Accuracy: 9582/10000 (96%)\n",
            "\n",
            "Train: [55680/60000 (93%)]\tLoss: 0.120734\n",
            "\n",
            "Test set: Average loss: 0.1432, Accuracy: 9604/10000 (96%)\n",
            "\n",
            "Train: [56320/60000 (94%)]\tLoss: 0.124384\n",
            "\n",
            "Test set: Average loss: 0.1469, Accuracy: 9593/10000 (96%)\n",
            "\n",
            "Train: [56960/60000 (95%)]\tLoss: 0.089812\n",
            "\n",
            "Test set: Average loss: 0.1436, Accuracy: 9592/10000 (96%)\n",
            "\n",
            "Train: [57600/60000 (96%)]\tLoss: 0.135476\n",
            "\n",
            "Test set: Average loss: 0.1431, Accuracy: 9588/10000 (96%)\n",
            "\n",
            "Train: [58240/60000 (97%)]\tLoss: 0.120989\n",
            "\n",
            "Test set: Average loss: 0.1432, Accuracy: 9605/10000 (96%)\n",
            "\n",
            "Train: [58880/60000 (98%)]\tLoss: 0.153436\n",
            "\n",
            "Test set: Average loss: 0.1444, Accuracy: 9601/10000 (96%)\n",
            "\n",
            "Train: [59520/60000 (99%)]\tLoss: 0.201857\n",
            "\n",
            "Test set: Average loss: 0.1392, Accuracy: 9604/10000 (96%)\n",
            "\n",
            "Train: [0/60000 (0%)]\tLoss: 2.308747\n",
            "\n",
            "Test set: Average loss: 8.6178, Accuracy: 2111/10000 (21%)\n",
            "\n",
            "Train: [640/60000 (1%)]\tLoss: 1.283418\n",
            "\n",
            "Test set: Average loss: 1.0495, Accuracy: 7247/10000 (72%)\n",
            "\n",
            "Train: [1280/60000 (2%)]\tLoss: 0.490672\n",
            "\n",
            "Test set: Average loss: 0.4176, Accuracy: 8836/10000 (88%)\n",
            "\n",
            "Train: [1920/60000 (3%)]\tLoss: 0.265265\n",
            "\n",
            "Test set: Average loss: 0.3178, Accuracy: 9073/10000 (91%)\n",
            "\n",
            "Train: [2560/60000 (4%)]\tLoss: 0.277315\n",
            "\n",
            "Test set: Average loss: 0.2539, Accuracy: 9298/10000 (93%)\n",
            "\n",
            "Train: [3200/60000 (5%)]\tLoss: 0.167370\n",
            "\n",
            "Test set: Average loss: 0.2405, Accuracy: 9280/10000 (93%)\n",
            "\n",
            "Train: [3840/60000 (6%)]\tLoss: 0.142961\n",
            "\n",
            "Test set: Average loss: 0.1978, Accuracy: 9434/10000 (94%)\n",
            "\n",
            "Train: [4480/60000 (7%)]\tLoss: 0.150638\n",
            "\n",
            "Test set: Average loss: 0.2110, Accuracy: 9337/10000 (93%)\n",
            "\n",
            "Train: [5120/60000 (9%)]\tLoss: 0.325755\n",
            "\n",
            "Test set: Average loss: 0.1679, Accuracy: 9523/10000 (95%)\n",
            "\n",
            "Train: [5760/60000 (10%)]\tLoss: 0.228969\n",
            "\n",
            "Test set: Average loss: 0.1709, Accuracy: 9476/10000 (95%)\n",
            "\n",
            "Train: [6400/60000 (11%)]\tLoss: 0.272303\n",
            "\n",
            "Test set: Average loss: 0.1393, Accuracy: 9579/10000 (96%)\n",
            "\n",
            "Train: [7040/60000 (12%)]\tLoss: 0.154076\n",
            "\n",
            "Test set: Average loss: 0.1395, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train: [7680/60000 (13%)]\tLoss: 0.064492\n",
            "\n",
            "Test set: Average loss: 0.1460, Accuracy: 9568/10000 (96%)\n",
            "\n",
            "Train: [8320/60000 (14%)]\tLoss: 0.053775\n",
            "\n",
            "Test set: Average loss: 0.1249, Accuracy: 9608/10000 (96%)\n",
            "\n",
            "Train: [8960/60000 (15%)]\tLoss: 0.164631\n",
            "\n",
            "Test set: Average loss: 0.1202, Accuracy: 9634/10000 (96%)\n",
            "\n",
            "Train: [9600/60000 (16%)]\tLoss: 0.212750\n",
            "\n",
            "Test set: Average loss: 0.1379, Accuracy: 9530/10000 (95%)\n",
            "\n",
            "Train: [10240/60000 (17%)]\tLoss: 0.089122\n",
            "\n",
            "Test set: Average loss: 0.1142, Accuracy: 9647/10000 (96%)\n",
            "\n",
            "Train: [10880/60000 (18%)]\tLoss: 0.085868\n",
            "\n",
            "Test set: Average loss: 0.1079, Accuracy: 9661/10000 (97%)\n",
            "\n",
            "Train: [11520/60000 (19%)]\tLoss: 0.230957\n",
            "\n",
            "Test set: Average loss: 0.1168, Accuracy: 9613/10000 (96%)\n",
            "\n",
            "Train: [12160/60000 (20%)]\tLoss: 0.100456\n",
            "\n",
            "Test set: Average loss: 0.0976, Accuracy: 9713/10000 (97%)\n",
            "\n",
            "Train: [12800/60000 (21%)]\tLoss: 0.059185\n",
            "\n",
            "Test set: Average loss: 0.1013, Accuracy: 9682/10000 (97%)\n",
            "\n",
            "Train: [13440/60000 (22%)]\tLoss: 0.138806\n",
            "\n",
            "Test set: Average loss: 0.0951, Accuracy: 9697/10000 (97%)\n",
            "\n",
            "Train: [14080/60000 (23%)]\tLoss: 0.024937\n",
            "\n",
            "Test set: Average loss: 0.0905, Accuracy: 9740/10000 (97%)\n",
            "\n",
            "Train: [14720/60000 (25%)]\tLoss: 0.067596\n",
            "\n",
            "Test set: Average loss: 0.0939, Accuracy: 9714/10000 (97%)\n",
            "\n",
            "Train: [15360/60000 (26%)]\tLoss: 0.283855\n",
            "\n",
            "Test set: Average loss: 0.1098, Accuracy: 9651/10000 (97%)\n",
            "\n",
            "Train: [16000/60000 (27%)]\tLoss: 0.145400\n",
            "\n",
            "Test set: Average loss: 0.0868, Accuracy: 9736/10000 (97%)\n",
            "\n",
            "Train: [16640/60000 (28%)]\tLoss: 0.082431\n",
            "\n",
            "Test set: Average loss: 0.1043, Accuracy: 9682/10000 (97%)\n",
            "\n",
            "Train: [17280/60000 (29%)]\tLoss: 0.098040\n",
            "\n",
            "Test set: Average loss: 0.0845, Accuracy: 9748/10000 (97%)\n",
            "\n",
            "Train: [17920/60000 (30%)]\tLoss: 0.059141\n",
            "\n",
            "Test set: Average loss: 0.0801, Accuracy: 9751/10000 (98%)\n",
            "\n",
            "Train: [18560/60000 (31%)]\tLoss: 0.040216\n",
            "\n",
            "Test set: Average loss: 0.0827, Accuracy: 9727/10000 (97%)\n",
            "\n",
            "Train: [19200/60000 (32%)]\tLoss: 0.140265\n",
            "\n",
            "Test set: Average loss: 0.0892, Accuracy: 9709/10000 (97%)\n",
            "\n",
            "Train: [19840/60000 (33%)]\tLoss: 0.101902\n",
            "\n",
            "Test set: Average loss: 0.0943, Accuracy: 9707/10000 (97%)\n",
            "\n",
            "Train: [20480/60000 (34%)]\tLoss: 0.153969\n",
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train: [21120/60000 (35%)]\tLoss: 0.091282\n",
            "\n",
            "Test set: Average loss: 0.0935, Accuracy: 9702/10000 (97%)\n",
            "\n",
            "Train: [21760/60000 (36%)]\tLoss: 0.067832\n",
            "\n",
            "Test set: Average loss: 0.0801, Accuracy: 9740/10000 (97%)\n",
            "\n",
            "Train: [22400/60000 (37%)]\tLoss: 0.045860\n",
            "\n",
            "Test set: Average loss: 0.0777, Accuracy: 9767/10000 (98%)\n",
            "\n",
            "Train: [23040/60000 (38%)]\tLoss: 0.189338\n",
            "\n",
            "Test set: Average loss: 0.0704, Accuracy: 9768/10000 (98%)\n",
            "\n",
            "Train: [23680/60000 (39%)]\tLoss: 0.050821\n",
            "\n",
            "Test set: Average loss: 0.0707, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train: [24320/60000 (41%)]\tLoss: 0.302370\n",
            "\n",
            "Test set: Average loss: 0.0701, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train: [24960/60000 (42%)]\tLoss: 0.051104\n",
            "\n",
            "Test set: Average loss: 0.0814, Accuracy: 9729/10000 (97%)\n",
            "\n",
            "Train: [25600/60000 (43%)]\tLoss: 0.023544\n",
            "\n",
            "Test set: Average loss: 0.0674, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train: [26240/60000 (44%)]\tLoss: 0.085811\n",
            "\n",
            "Test set: Average loss: 0.0697, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train: [26880/60000 (45%)]\tLoss: 0.123243\n",
            "\n",
            "Test set: Average loss: 0.0688, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train: [27520/60000 (46%)]\tLoss: 0.074743\n",
            "\n",
            "Test set: Average loss: 0.0642, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "Train: [28160/60000 (47%)]\tLoss: 0.042059\n",
            "\n",
            "Test set: Average loss: 0.0620, Accuracy: 9810/10000 (98%)\n",
            "\n",
            "Train: [28800/60000 (48%)]\tLoss: 0.103427\n",
            "\n",
            "Test set: Average loss: 0.0682, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train: [29440/60000 (49%)]\tLoss: 0.039658\n",
            "\n",
            "Test set: Average loss: 0.0579, Accuracy: 9827/10000 (98%)\n",
            "\n",
            "Train: [30080/60000 (50%)]\tLoss: 0.047075\n",
            "\n",
            "Test set: Average loss: 0.0694, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train: [30720/60000 (51%)]\tLoss: 0.052713\n",
            "\n",
            "Test set: Average loss: 0.0662, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train: [31360/60000 (52%)]\tLoss: 0.185793\n",
            "\n",
            "Test set: Average loss: 0.0654, Accuracy: 9793/10000 (98%)\n",
            "\n",
            "Train: [32000/60000 (53%)]\tLoss: 0.131928\n",
            "\n",
            "Test set: Average loss: 0.0626, Accuracy: 9795/10000 (98%)\n",
            "\n",
            "Train: [32640/60000 (54%)]\tLoss: 0.019707\n",
            "\n",
            "Test set: Average loss: 0.0633, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train: [33280/60000 (55%)]\tLoss: 0.021892\n",
            "\n",
            "Test set: Average loss: 0.0674, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train: [33920/60000 (57%)]\tLoss: 0.027367\n",
            "\n",
            "Test set: Average loss: 0.0712, Accuracy: 9774/10000 (98%)\n",
            "\n",
            "Train: [34560/60000 (58%)]\tLoss: 0.192708\n",
            "\n",
            "Test set: Average loss: 0.0681, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train: [35200/60000 (59%)]\tLoss: 0.005980\n",
            "\n",
            "Test set: Average loss: 0.0643, Accuracy: 9809/10000 (98%)\n",
            "\n",
            "Train: [35840/60000 (60%)]\tLoss: 0.088829\n",
            "\n",
            "Test set: Average loss: 0.0649, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train: [36480/60000 (61%)]\tLoss: 0.049990\n",
            "\n",
            "Test set: Average loss: 0.0581, Accuracy: 9808/10000 (98%)\n",
            "\n",
            "Train: [37120/60000 (62%)]\tLoss: 0.018863\n",
            "\n",
            "Test set: Average loss: 0.0615, Accuracy: 9797/10000 (98%)\n",
            "\n",
            "Train: [37760/60000 (63%)]\tLoss: 0.046894\n",
            "\n",
            "Test set: Average loss: 0.0612, Accuracy: 9804/10000 (98%)\n",
            "\n",
            "Train: [38400/60000 (64%)]\tLoss: 0.058133\n",
            "\n",
            "Test set: Average loss: 0.0728, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Train: [39040/60000 (65%)]\tLoss: 0.065451\n",
            "\n",
            "Test set: Average loss: 0.0591, Accuracy: 9803/10000 (98%)\n",
            "\n",
            "Train: [39680/60000 (66%)]\tLoss: 0.099744\n",
            "\n",
            "Test set: Average loss: 0.0688, Accuracy: 9746/10000 (97%)\n",
            "\n",
            "Train: [40320/60000 (67%)]\tLoss: 0.045213\n",
            "\n",
            "Test set: Average loss: 0.0576, Accuracy: 9821/10000 (98%)\n",
            "\n",
            "Train: [40960/60000 (68%)]\tLoss: 0.041035\n",
            "\n",
            "Test set: Average loss: 0.0581, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train: [41600/60000 (69%)]\tLoss: 0.066705\n",
            "\n",
            "Test set: Average loss: 0.0597, Accuracy: 9805/10000 (98%)\n",
            "\n",
            "Train: [42240/60000 (70%)]\tLoss: 0.029431\n",
            "\n",
            "Test set: Average loss: 0.0692, Accuracy: 9789/10000 (98%)\n",
            "\n",
            "Train: [42880/60000 (71%)]\tLoss: 0.084197\n",
            "\n",
            "Test set: Average loss: 0.0576, Accuracy: 9822/10000 (98%)\n",
            "\n",
            "Train: [43520/60000 (72%)]\tLoss: 0.015681\n",
            "\n",
            "Test set: Average loss: 0.0614, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train: [44160/60000 (74%)]\tLoss: 0.045185\n",
            "\n",
            "Test set: Average loss: 0.0595, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train: [44800/60000 (75%)]\tLoss: 0.133207\n",
            "\n",
            "Test set: Average loss: 0.0599, Accuracy: 9796/10000 (98%)\n",
            "\n",
            "Train: [45440/60000 (76%)]\tLoss: 0.036735\n",
            "\n",
            "Test set: Average loss: 0.0546, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train: [46080/60000 (77%)]\tLoss: 0.105692\n",
            "\n",
            "Test set: Average loss: 0.0676, Accuracy: 9777/10000 (98%)\n",
            "\n",
            "Train: [46720/60000 (78%)]\tLoss: 0.076610\n",
            "\n",
            "Test set: Average loss: 0.0699, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train: [47360/60000 (79%)]\tLoss: 0.049016\n",
            "\n",
            "Test set: Average loss: 0.0566, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train: [48000/60000 (80%)]\tLoss: 0.104935\n",
            "\n",
            "Test set: Average loss: 0.0667, Accuracy: 9790/10000 (98%)\n",
            "\n",
            "Train: [48640/60000 (81%)]\tLoss: 0.007672\n",
            "\n",
            "Test set: Average loss: 0.0533, Accuracy: 9833/10000 (98%)\n",
            "\n",
            "Train: [49280/60000 (82%)]\tLoss: 0.042916\n",
            "\n",
            "Test set: Average loss: 0.0624, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train: [49920/60000 (83%)]\tLoss: 0.024257\n",
            "\n",
            "Test set: Average loss: 0.0560, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train: [50560/60000 (84%)]\tLoss: 0.042463\n",
            "\n",
            "Test set: Average loss: 0.0569, Accuracy: 9810/10000 (98%)\n",
            "\n",
            "Train: [51200/60000 (85%)]\tLoss: 0.074611\n",
            "\n",
            "Test set: Average loss: 0.0568, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Train: [51840/60000 (86%)]\tLoss: 0.186188\n",
            "\n",
            "Test set: Average loss: 0.0553, Accuracy: 9813/10000 (98%)\n",
            "\n",
            "Train: [52480/60000 (87%)]\tLoss: 0.013837\n",
            "\n",
            "Test set: Average loss: 0.0518, Accuracy: 9836/10000 (98%)\n",
            "\n",
            "Train: [53120/60000 (88%)]\tLoss: 0.037273\n",
            "\n",
            "Test set: Average loss: 0.0608, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train: [53760/60000 (90%)]\tLoss: 0.095917\n",
            "\n",
            "Test set: Average loss: 0.0543, Accuracy: 9827/10000 (98%)\n",
            "\n",
            "Train: [54400/60000 (91%)]\tLoss: 0.010752\n",
            "\n",
            "Test set: Average loss: 0.0469, Accuracy: 9852/10000 (99%)\n",
            "\n",
            "Train: [55040/60000 (92%)]\tLoss: 0.065322\n",
            "\n",
            "Test set: Average loss: 0.0692, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Train: [55680/60000 (93%)]\tLoss: 0.037509\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 9838/10000 (98%)\n",
            "\n",
            "Train: [56320/60000 (94%)]\tLoss: 0.094981\n",
            "\n",
            "Test set: Average loss: 0.0458, Accuracy: 9857/10000 (99%)\n",
            "\n",
            "Train: [56960/60000 (95%)]\tLoss: 0.018878\n",
            "\n",
            "Test set: Average loss: 0.0520, Accuracy: 9832/10000 (98%)\n",
            "\n",
            "Train: [57600/60000 (96%)]\tLoss: 0.012840\n",
            "\n",
            "Test set: Average loss: 0.0476, Accuracy: 9843/10000 (98%)\n",
            "\n",
            "Train: [58240/60000 (97%)]\tLoss: 0.109365\n",
            "\n",
            "Test set: Average loss: 0.0743, Accuracy: 9744/10000 (97%)\n",
            "\n",
            "Train: [58880/60000 (98%)]\tLoss: 0.090695\n",
            "\n",
            "Test set: Average loss: 0.0482, Accuracy: 9850/10000 (98%)\n",
            "\n",
            "Train: [59520/60000 (99%)]\tLoss: 0.025774\n",
            "\n",
            "Test set: Average loss: 0.0459, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "Train: [0/60000 (0%)]\tLoss: 2.317504\n",
            "\n",
            "Test set: Average loss: 2.2287, Accuracy: 1784/10000 (18%)\n",
            "\n",
            "Train: [640/60000 (1%)]\tLoss: 0.707849\n",
            "\n",
            "Test set: Average loss: 0.6649, Accuracy: 7875/10000 (79%)\n",
            "\n",
            "Train: [1280/60000 (2%)]\tLoss: 0.285454\n",
            "\n",
            "Test set: Average loss: 0.4602, Accuracy: 8608/10000 (86%)\n",
            "\n",
            "Train: [1920/60000 (3%)]\tLoss: 0.571943\n",
            "\n",
            "Test set: Average loss: 0.3597, Accuracy: 8952/10000 (90%)\n",
            "\n",
            "Train: [2560/60000 (4%)]\tLoss: 0.390265\n",
            "\n",
            "Test set: Average loss: 0.3106, Accuracy: 9123/10000 (91%)\n",
            "\n",
            "Train: [3200/60000 (5%)]\tLoss: 0.303328\n",
            "\n",
            "Test set: Average loss: 0.2482, Accuracy: 9267/10000 (93%)\n",
            "\n",
            "Train: [3840/60000 (6%)]\tLoss: 0.386824\n",
            "\n",
            "Test set: Average loss: 0.2248, Accuracy: 9317/10000 (93%)\n",
            "\n",
            "Train: [4480/60000 (7%)]\tLoss: 0.180542\n",
            "\n",
            "Test set: Average loss: 0.2110, Accuracy: 9371/10000 (94%)\n",
            "\n",
            "Train: [5120/60000 (9%)]\tLoss: 0.298124\n",
            "\n",
            "Test set: Average loss: 0.1863, Accuracy: 9451/10000 (95%)\n",
            "\n",
            "Train: [5760/60000 (10%)]\tLoss: 0.086087\n",
            "\n",
            "Test set: Average loss: 0.1430, Accuracy: 9600/10000 (96%)\n",
            "\n",
            "Train: [6400/60000 (11%)]\tLoss: 0.067795\n",
            "\n",
            "Test set: Average loss: 0.1450, Accuracy: 9559/10000 (96%)\n",
            "\n",
            "Train: [7040/60000 (12%)]\tLoss: 0.317017\n",
            "\n",
            "Test set: Average loss: 0.1323, Accuracy: 9591/10000 (96%)\n",
            "\n",
            "Train: [7680/60000 (13%)]\tLoss: 0.058683\n",
            "\n",
            "Test set: Average loss: 0.1185, Accuracy: 9645/10000 (96%)\n",
            "\n",
            "Train: [8320/60000 (14%)]\tLoss: 0.109865\n",
            "\n",
            "Test set: Average loss: 0.1265, Accuracy: 9604/10000 (96%)\n",
            "\n",
            "Train: [8960/60000 (15%)]\tLoss: 0.170272\n",
            "\n",
            "Test set: Average loss: 0.1092, Accuracy: 9702/10000 (97%)\n",
            "\n",
            "Train: [9600/60000 (16%)]\tLoss: 0.108709\n",
            "\n",
            "Test set: Average loss: 0.0964, Accuracy: 9712/10000 (97%)\n",
            "\n",
            "Train: [10240/60000 (17%)]\tLoss: 0.063107\n",
            "\n",
            "Test set: Average loss: 0.1013, Accuracy: 9661/10000 (97%)\n",
            "\n",
            "Train: [10880/60000 (18%)]\tLoss: 0.112011\n",
            "\n",
            "Test set: Average loss: 0.0919, Accuracy: 9737/10000 (97%)\n",
            "\n",
            "Train: [11520/60000 (19%)]\tLoss: 0.048574\n",
            "\n",
            "Test set: Average loss: 0.0852, Accuracy: 9741/10000 (97%)\n",
            "\n",
            "Train: [12160/60000 (20%)]\tLoss: 0.084594\n",
            "\n",
            "Test set: Average loss: 0.0922, Accuracy: 9723/10000 (97%)\n",
            "\n",
            "Train: [12800/60000 (21%)]\tLoss: 0.059449\n",
            "\n",
            "Test set: Average loss: 0.0800, Accuracy: 9755/10000 (98%)\n",
            "\n",
            "Train: [13440/60000 (22%)]\tLoss: 0.096926\n",
            "\n",
            "Test set: Average loss: 0.0916, Accuracy: 9739/10000 (97%)\n",
            "\n",
            "Train: [14080/60000 (23%)]\tLoss: 0.042465\n",
            "\n",
            "Test set: Average loss: 0.0761, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train: [14720/60000 (25%)]\tLoss: 0.064242\n",
            "\n",
            "Test set: Average loss: 0.0786, Accuracy: 9761/10000 (98%)\n",
            "\n",
            "Train: [15360/60000 (26%)]\tLoss: 0.018409\n",
            "\n",
            "Test set: Average loss: 0.1000, Accuracy: 9707/10000 (97%)\n",
            "\n",
            "Train: [16000/60000 (27%)]\tLoss: 0.118181\n",
            "\n",
            "Test set: Average loss: 0.0964, Accuracy: 9711/10000 (97%)\n",
            "\n",
            "Train: [16640/60000 (28%)]\tLoss: 0.032578\n",
            "\n",
            "Test set: Average loss: 0.0794, Accuracy: 9763/10000 (98%)\n",
            "\n",
            "Train: [17280/60000 (29%)]\tLoss: 0.105286\n",
            "\n",
            "Test set: Average loss: 0.0884, Accuracy: 9737/10000 (97%)\n",
            "\n",
            "Train: [17920/60000 (30%)]\tLoss: 0.050729\n",
            "\n",
            "Test set: Average loss: 0.0771, Accuracy: 9756/10000 (98%)\n",
            "\n",
            "Train: [18560/60000 (31%)]\tLoss: 0.094144\n",
            "\n",
            "Test set: Average loss: 0.0803, Accuracy: 9764/10000 (98%)\n",
            "\n",
            "Train: [19200/60000 (32%)]\tLoss: 0.137118\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train: [19840/60000 (33%)]\tLoss: 0.095665\n",
            "\n",
            "Test set: Average loss: 0.0684, Accuracy: 9780/10000 (98%)\n",
            "\n",
            "Train: [20480/60000 (34%)]\tLoss: 0.022409\n",
            "\n",
            "Test set: Average loss: 0.0699, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train: [21120/60000 (35%)]\tLoss: 0.218075\n",
            "\n",
            "Test set: Average loss: 0.0626, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train: [21760/60000 (36%)]\tLoss: 0.130563\n",
            "\n",
            "Test set: Average loss: 0.0649, Accuracy: 9794/10000 (98%)\n",
            "\n",
            "Train: [22400/60000 (37%)]\tLoss: 0.151126\n",
            "\n",
            "Test set: Average loss: 0.0578, Accuracy: 9826/10000 (98%)\n",
            "\n",
            "Train: [23040/60000 (38%)]\tLoss: 0.068398\n",
            "\n",
            "Test set: Average loss: 0.0679, Accuracy: 9786/10000 (98%)\n",
            "\n",
            "Train: [23680/60000 (39%)]\tLoss: 0.060307\n",
            "\n",
            "Test set: Average loss: 0.0748, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train: [24320/60000 (41%)]\tLoss: 0.056731\n",
            "\n",
            "Test set: Average loss: 0.0644, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train: [24960/60000 (42%)]\tLoss: 0.087321\n",
            "\n",
            "Test set: Average loss: 0.0702, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train: [25600/60000 (43%)]\tLoss: 0.120580\n",
            "\n",
            "Test set: Average loss: 0.0698, Accuracy: 9769/10000 (98%)\n",
            "\n",
            "Train: [26240/60000 (44%)]\tLoss: 0.020590\n",
            "\n",
            "Test set: Average loss: 0.0705, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train: [26880/60000 (45%)]\tLoss: 0.101850\n",
            "\n",
            "Test set: Average loss: 0.0665, Accuracy: 9791/10000 (98%)\n",
            "\n",
            "Train: [27520/60000 (46%)]\tLoss: 0.019465\n",
            "\n",
            "Test set: Average loss: 0.0659, Accuracy: 9787/10000 (98%)\n",
            "\n",
            "Train: [28160/60000 (47%)]\tLoss: 0.091073\n",
            "\n",
            "Test set: Average loss: 0.0632, Accuracy: 9810/10000 (98%)\n",
            "\n",
            "Train: [28800/60000 (48%)]\tLoss: 0.074891\n",
            "\n",
            "Test set: Average loss: 0.0776, Accuracy: 9748/10000 (97%)\n",
            "\n",
            "Train: [29440/60000 (49%)]\tLoss: 0.034615\n",
            "\n",
            "Test set: Average loss: 0.0684, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train: [30080/60000 (50%)]\tLoss: 0.079953\n",
            "\n",
            "Test set: Average loss: 0.0644, Accuracy: 9785/10000 (98%)\n",
            "\n",
            "Train: [30720/60000 (51%)]\tLoss: 0.055679\n",
            "\n",
            "Test set: Average loss: 0.0593, Accuracy: 9809/10000 (98%)\n",
            "\n",
            "Train: [31360/60000 (52%)]\tLoss: 0.024596\n",
            "\n",
            "Test set: Average loss: 0.0455, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "Train: [32000/60000 (53%)]\tLoss: 0.071926\n",
            "\n",
            "Test set: Average loss: 0.0590, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train: [32640/60000 (54%)]\tLoss: 0.070842\n",
            "\n",
            "Test set: Average loss: 0.0580, Accuracy: 9816/10000 (98%)\n",
            "\n",
            "Train: [33280/60000 (55%)]\tLoss: 0.005813\n",
            "\n",
            "Test set: Average loss: 0.0521, Accuracy: 9832/10000 (98%)\n",
            "\n",
            "Train: [33920/60000 (57%)]\tLoss: 0.048228\n",
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 9833/10000 (98%)\n",
            "\n",
            "Train: [34560/60000 (58%)]\tLoss: 0.051768\n",
            "\n",
            "Test set: Average loss: 0.0575, Accuracy: 9821/10000 (98%)\n",
            "\n",
            "Train: [35200/60000 (59%)]\tLoss: 0.011871\n",
            "\n",
            "Test set: Average loss: 0.0546, Accuracy: 9843/10000 (98%)\n",
            "\n",
            "Train: [35840/60000 (60%)]\tLoss: 0.034820\n",
            "\n",
            "Test set: Average loss: 0.0519, Accuracy: 9846/10000 (98%)\n",
            "\n",
            "Train: [36480/60000 (61%)]\tLoss: 0.052234\n",
            "\n",
            "Test set: Average loss: 0.0502, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train: [37120/60000 (62%)]\tLoss: 0.061015\n",
            "\n",
            "Test set: Average loss: 0.0479, Accuracy: 9857/10000 (99%)\n",
            "\n",
            "Train: [37760/60000 (63%)]\tLoss: 0.014760\n",
            "\n",
            "Test set: Average loss: 0.0464, Accuracy: 9862/10000 (99%)\n",
            "\n",
            "Train: [38400/60000 (64%)]\tLoss: 0.096770\n",
            "\n",
            "Test set: Average loss: 0.0483, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train: [39040/60000 (65%)]\tLoss: 0.058893\n",
            "\n",
            "Test set: Average loss: 0.0574, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train: [39680/60000 (66%)]\tLoss: 0.031114\n",
            "\n",
            "Test set: Average loss: 0.0551, Accuracy: 9812/10000 (98%)\n",
            "\n",
            "Train: [40320/60000 (67%)]\tLoss: 0.020189\n",
            "\n",
            "Test set: Average loss: 0.0457, Accuracy: 9847/10000 (98%)\n",
            "\n",
            "Train: [40960/60000 (68%)]\tLoss: 0.042762\n",
            "\n",
            "Test set: Average loss: 0.0638, Accuracy: 9798/10000 (98%)\n",
            "\n",
            "Train: [41600/60000 (69%)]\tLoss: 0.178978\n",
            "\n",
            "Test set: Average loss: 0.0719, Accuracy: 9771/10000 (98%)\n",
            "\n",
            "Train: [42240/60000 (70%)]\tLoss: 0.162804\n",
            "\n",
            "Test set: Average loss: 0.0728, Accuracy: 9758/10000 (98%)\n",
            "\n",
            "Train: [42880/60000 (71%)]\tLoss: 0.041359\n",
            "\n",
            "Test set: Average loss: 0.0483, Accuracy: 9835/10000 (98%)\n",
            "\n",
            "Train: [43520/60000 (72%)]\tLoss: 0.147101\n",
            "\n",
            "Test set: Average loss: 0.0559, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "Train: [44160/60000 (74%)]\tLoss: 0.056948\n",
            "\n",
            "Test set: Average loss: 0.0461, Accuracy: 9851/10000 (99%)\n",
            "\n",
            "Train: [44800/60000 (75%)]\tLoss: 0.009886\n",
            "\n",
            "Test set: Average loss: 0.0506, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train: [45440/60000 (76%)]\tLoss: 0.003899\n",
            "\n",
            "Test set: Average loss: 0.0466, Accuracy: 9849/10000 (98%)\n",
            "\n",
            "Train: [46080/60000 (77%)]\tLoss: 0.041747\n",
            "\n",
            "Test set: Average loss: 0.0498, Accuracy: 9842/10000 (98%)\n",
            "\n",
            "Train: [46720/60000 (78%)]\tLoss: 0.016818\n",
            "\n",
            "Test set: Average loss: 0.0528, Accuracy: 9842/10000 (98%)\n",
            "\n",
            "Train: [47360/60000 (79%)]\tLoss: 0.114312\n",
            "\n",
            "Test set: Average loss: 0.0496, Accuracy: 9837/10000 (98%)\n",
            "\n",
            "Train: [48000/60000 (80%)]\tLoss: 0.069929\n",
            "\n",
            "Test set: Average loss: 0.0514, Accuracy: 9834/10000 (98%)\n",
            "\n",
            "Train: [48640/60000 (81%)]\tLoss: 0.101188\n",
            "\n",
            "Test set: Average loss: 0.0461, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train: [49280/60000 (82%)]\tLoss: 0.064127\n",
            "\n",
            "Test set: Average loss: 0.0562, Accuracy: 9818/10000 (98%)\n",
            "\n",
            "Train: [49920/60000 (83%)]\tLoss: 0.179743\n",
            "\n",
            "Test set: Average loss: 0.0406, Accuracy: 9870/10000 (99%)\n",
            "\n",
            "Train: [50560/60000 (84%)]\tLoss: 0.026727\n",
            "\n",
            "Test set: Average loss: 0.0577, Accuracy: 9815/10000 (98%)\n",
            "\n",
            "Train: [51200/60000 (85%)]\tLoss: 0.102528\n",
            "\n",
            "Test set: Average loss: 0.0524, Accuracy: 9832/10000 (98%)\n",
            "\n",
            "Train: [51840/60000 (86%)]\tLoss: 0.093837\n",
            "\n",
            "Test set: Average loss: 0.0450, Accuracy: 9859/10000 (99%)\n",
            "\n",
            "Train: [52480/60000 (87%)]\tLoss: 0.017914\n",
            "\n",
            "Test set: Average loss: 0.0446, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "Train: [53120/60000 (88%)]\tLoss: 0.074803\n",
            "\n",
            "Test set: Average loss: 0.0518, Accuracy: 9825/10000 (98%)\n",
            "\n",
            "Train: [53760/60000 (90%)]\tLoss: 0.161680\n",
            "\n",
            "Test set: Average loss: 0.0488, Accuracy: 9840/10000 (98%)\n",
            "\n",
            "Train: [54400/60000 (91%)]\tLoss: 0.062222\n",
            "\n",
            "Test set: Average loss: 0.0494, Accuracy: 9834/10000 (98%)\n",
            "\n",
            "Train: [55040/60000 (92%)]\tLoss: 0.037299\n",
            "\n",
            "Test set: Average loss: 0.0503, Accuracy: 9838/10000 (98%)\n",
            "\n",
            "Train: [55680/60000 (93%)]\tLoss: 0.004456\n",
            "\n",
            "Test set: Average loss: 0.0474, Accuracy: 9846/10000 (98%)\n",
            "\n",
            "Train: [56320/60000 (94%)]\tLoss: 0.063204\n",
            "\n",
            "Test set: Average loss: 0.0417, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "Train: [56960/60000 (95%)]\tLoss: 0.047206\n",
            "\n",
            "Test set: Average loss: 0.0457, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train: [57600/60000 (96%)]\tLoss: 0.050256\n",
            "\n",
            "Test set: Average loss: 0.0465, Accuracy: 9858/10000 (99%)\n",
            "\n",
            "Train: [58240/60000 (97%)]\tLoss: 0.060884\n",
            "\n",
            "Test set: Average loss: 0.0676, Accuracy: 9778/10000 (98%)\n",
            "\n",
            "Train: [58880/60000 (98%)]\tLoss: 0.047585\n",
            "\n",
            "Test set: Average loss: 0.0685, Accuracy: 9779/10000 (98%)\n",
            "\n",
            "Train: [59520/60000 (99%)]\tLoss: 0.037248\n",
            "\n",
            "Test set: Average loss: 0.0553, Accuracy: 9835/10000 (98%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cuda_kwargs = {'num_workers': 1, 'pin_memory': True, 'shuffle': True}\n",
        "train_kwargs.update(cuda_kwargs)\n",
        "test_kwargs.update(cuda_kwargs)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "# Instantiate model\n",
        "model = Net().to(device)\n",
        "log_interval = 10\n",
        "\n",
        "# Training with different optimizers\n",
        "for optimizer_name, optimizer_class in [(\"adagrad\", adagrad_optimizer), (\"rmsprop\", rmsprop_optimizer), (\"adam\", adam_optimizer)]:\n",
        "    model = Net().to(device)  # Re-initialize model for each optimizer\n",
        "    optimizer = optimizer_class(model)  # Instantiate optimizer\n",
        "    train(log_interval, model, device, train_loader, test_loader, optimizer)\n",
        "    torch.save(model.state_dict(), f\"mnist_cnn_{optimizer_name}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After running through the model i've got the following results :\n",
        "\n",
        "\n",
        "adgrad_optimizer is giving Accuracy of 96%\n",
        "\n",
        "rmsprop_optimizer is giving Accuracy of 98%\n",
        "\n",
        "adam_optimizer is giving Accuracy of 98 %"
      ],
      "metadata": {
        "id": "XeKdj9LcBa_P"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}